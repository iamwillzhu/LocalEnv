{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook outlines the steps involved in building and deploying a Battlesnake model using Ray RLlib and TensorFlow on Amazon SageMaker.\n",
    "\n",
    "Library versions currently in use:  TensorFlow 2.1, Ray RLlib 0.8.2\n",
    "\n",
    "The model is first trained using multi-agent PPO, and then deployed to a managed _TensorFlow Serving_ SageMaker endpoint that can be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.rl import RLEstimator, RLToolkit\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise sagemaker\n",
    "We need to define several parameters prior to running the training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://sagemaker-us-west-2-216604823851/\n"
     ]
    }
   ],
   "source": [
    "sm_session = sagemaker.session.Session()\n",
    "s3_bucket = sm_session.default_bucket()\n",
    "\n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::216604823851:role/BattlesnakeEnvironment-NotebookInstanceExecutionRo-YMEAYYBNHRKI\n"
     ]
    }
   ],
   "source": [
    "job_name_prefix = 'Battlesnake-job-rllib'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change local_mode to True if you want to do local training within this Notebook instance\n",
    "# Otherwise, we'll spin-up a SageMaker training instance to handle the training\n",
    "\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = 'local'\n",
    "else:\n",
    "    instance_type = \"ml.m5.xlarge\"\n",
    "    \n",
    "# If training locally, do some Docker housekeeping..\n",
    "if local_mode:\n",
    "    !/bin/bash ./common/setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sm_session.boto_region_name\n",
    "device = \"cpu\"\n",
    "image_name = '462105765813.dkr.ecr.{region}.amazonaws.com/sagemaker-rl-ray-container:ray-0.8.2-tf-{device}-py36'.format(region=region, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-16 21:17:19 Starting - Starting the training job...\n",
      "2020-07-16 21:17:21 Starting - Launching requested ML instances......\n",
      "2020-07-16 21:18:26 Starting - Preparing the instances for training...\n",
      "2020-07-16 21:19:13 Downloading - Downloading input data\n",
      "2020-07-16 21:19:13 Training - Downloading the training image......\n",
      "2020-07-16 21:20:19 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-07-16 21:20:23,114 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-07-16 21:20:23,120 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-16 21:20:23,233 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting array2gif\n",
      "  Downloading array2gif-1.0.4-py3-none-any.whl (7.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting mxboard\n",
      "  Downloading mxboard-0.1.0-py3-none-any.whl (47 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from array2gif->-r requirements.txt (line 1)) (1.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from mxboard->-r requirements.txt (line 2)) (7.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from mxboard->-r requirements.txt (line 2)) (3.11.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from mxboard->-r requirements.txt (line 2)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 3)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 3)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->-r requirements.txt (line 3)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.0.0->mxboard->-r requirements.txt (line 2)) (46.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (1.25.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->-r requirements.txt (line 3)) (3.0.4)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: battlesnake-gym\n",
      "  Building wheel for battlesnake-gym (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for battlesnake-gym (setup.py): finished with status 'done'\n",
      "  Created wheel for battlesnake-gym: filename=battlesnake_gym-0.1.dev0-py3-none-any.whl size=16618 sha256=22196debeba78158b1e0496e208fce5b7c9755aadd2537823c481b131f6e7984\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-o4a2gico/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\u001b[0m\n",
      "\u001b[34mSuccessfully built battlesnake-gym\u001b[0m\n",
      "\u001b[34mInstalling collected packages: array2gif, mxboard, battlesnake-gym\n",
      "  Attempting uninstall: battlesnake-gym\n",
      "    Found existing installation: battlesnake-gym 0.1.dev0\n",
      "    Can't uninstall 'battlesnake-gym'. No files were found to uninstall.\u001b[0m\n",
      "\u001b[34mSuccessfully installed array2gif-1.0.4 battlesnake-gym-0.1.dev0 mxboard-0.1.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-07-16 21:20:25,490 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-16 21:20:25,505 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-16 21:20:25,519 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-07-16 21:20:25,529 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_estimator\": \"RLEstimator\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"num_iters\": 30,\n",
      "        \"num_agents\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"Battlesnake-job-rllib-2020-07-16-21-17-19-254\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-216604823851/Battlesnake-job-rllib-2020-07-16-21-17-19-254/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-mabs\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-mabs.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"num_agents\":5,\"num_iters\":30}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-mabs.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator\":\"RLEstimator\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-mabs\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-216604823851/Battlesnake-job-rllib-2020-07-16-21-17-19-254/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"num_agents\":5,\"num_iters\":30},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"Battlesnake-job-rllib-2020-07-16-21-17-19-254\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-216604823851/Battlesnake-job-rllib-2020-07-16-21-17-19-254/source/sourcedir.tar.gz\",\"module_name\":\"train-mabs\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-mabs.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--num_agents\",\"5\",\"--num_iters\",\"30\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ITERS=30\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_AGENTS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 -m train-mabs --num_agents 5 --num_iters 30\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-16 21:20:28,448#011INFO resource_spec.py:212 -- Starting Ray with 6.69 GiB memory available for workers and up to 3.36 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\u001b[0m\n",
      "\u001b[34m2020-07-16 21:20:28,852#011INFO services.py:1078 -- View the Ray dashboard at #033[1m#033[32mlocalhost:8265#033[39m#033[22m\u001b[0m\n",
      "\u001b[34mNo checkpoint path specified. Training from scratch.\u001b[0m\n",
      "\u001b[34mImportant! Ray with version <=7.2 may report \"Did not find checkpoint file\" even if the experiment is actually restored successfully. If restoration is expected, please check \"training_iteration\" in the experiment info to confirm.\u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 2.1/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+-------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc   |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+-------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  |       |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+-------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-16 21:20:30,585#011WARNING worker.py:1058 -- The dashboard on node ip-10-0-157-251.us-west-2.compute.internal failed with the following error:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1062, in create_server\n",
      "    sock.bind(sa)\u001b[0m\n",
      "\u001b[34mOSError: [Errno 99] Cannot assign requested address\n",
      "\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/dashboard/dashboard.py\", line 920, in <module>\n",
      "    dashboard.run()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/dashboard/dashboard.py\", line 368, in run\n",
      "    aiohttp.web.run_app(self.app, host=self.host, port=self.port)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web.py\", line 433, in run_app\n",
      "    reuse_port=reuse_port))\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 484, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web.py\", line 359, in _run_app\n",
      "    await site.start()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/aiohttp/web_runner.py\", line 104, in start\n",
      "    reuse_port=self._reuse_port)\n",
      "  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1066, in create_server\n",
      "    % (sa, err.strerror.lower()))\u001b[0m\n",
      "\u001b[34mOSError: [Errno 99] error while attempting to bind on address ('::1', 8265, 0, 0): cannot assign requested address\n",
      "\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m 2020-07-16 21:20:32,428#011INFO trainer.py:420 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m 2020-07-16 21:20:32,430#011INFO trainer.py:580 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m   obj = yaml.load(type_)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=116)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=115)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=113)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=114)#033[0m 2020-07-16 21:21:08,786#011INFO trainable.py:178 -- _setup took 36.358 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-22-31\n",
      "  done: false\n",
      "  episode_len_mean: 3.624168947985921\n",
      "  episode_reward_max: 1.9500000000000002\n",
      "  episode_reward_mean: -1.6056315995307002\n",
      "  episode_reward_min: -9.6\n",
      "  episodes_this_iter: 2557\n",
      "  episodes_total: 2557\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 18696.849\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3696852922439575\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009921830147504807\n",
      "        policy_loss: -0.012538837268948555\n",
      "        total_loss: 1.6771581172943115\n",
      "        vf_explained_var: 0.17832517623901367\n",
      "        vf_loss: 1.6877124309539795\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3597750663757324\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01386078167706728\n",
      "        policy_loss: -0.02538066729903221\n",
      "        total_loss: 1.6258726119995117\n",
      "        vf_explained_var: 0.17420794069766998\n",
      "        vf_loss: 1.648481011390686\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3637216091156006\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010725840926170349\n",
      "        policy_loss: -0.015353728085756302\n",
      "        total_loss: 1.6572858095169067\n",
      "        vf_explained_var: 0.16451522707939148\n",
      "        vf_loss: 1.6704943180084229\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.356479525566101\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01384509913623333\n",
      "        policy_loss: -0.019493797793984413\n",
      "        total_loss: 1.8507131338119507\n",
      "        vf_explained_var: 0.18062788248062134\n",
      "        vf_loss: 1.8674378395080566\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3703888654708862\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013246932998299599\n",
      "        policy_loss: -0.022643933072686195\n",
      "        total_loss: 2.0237042903900146\n",
      "        vf_explained_var: 0.14906412363052368\n",
      "        vf_loss: 2.043698787689209\n",
      "    load_time_ms: 2678.645\n",
      "    num_steps_sampled: 9267\n",
      "    num_steps_trained: 9216\n",
      "    sample_time_ms: 34753.062\n",
      "    update_time_ms: 22503.974\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.498245614035085\n",
      "    ram_util_percent: 33.574561403508774\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 5.5\n",
      "    policy_1: 5.3999999999999995\n",
      "    policy_2: 5.45\n",
      "    policy_3: 5.45\n",
      "    policy_4: 5.5\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.4470082127493156\n",
      "    policy_1: -0.42328901055924906\n",
      "    policy_2: -0.37307391474384044\n",
      "    policy_3: -0.2006843957763004\n",
      "    policy_4: -0.16157606570199456\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.6426372034188679\n",
      "    mean_inference_ms: 6.452290106742203\n",
      "    mean_processing_ms: 2.2833240666766277\n",
      "  sm__effective_map_size: 7\n",
      "  sm__episode_len_mean: 3.624168947985921\n",
      "  sm__episode_reward_max: 1.9500000000000002\n",
      "  sm__episode_reward_mean: -1.6056315995307002\n",
      "  sm__episode_reward_min: -9.6\n",
      "  sm__policy_0_reward_max: 5.5\n",
      "  sm__policy_0_reward_mean: -0.4470082127493156\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 80.14597344398499\n",
      "  time_this_iter_s: 80.14597344398499\n",
      "  time_total_s: 80.14597344398499\n",
      "  timestamp: 1594934551\n",
      "  timesteps_since_restore: 9267\n",
      "  timesteps_this_iter: 9267\n",
      "  timesteps_total: 9267\n",
      "  training_iteration: 1\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.2/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |   ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 | -1.60563 |           80.146 | 9267 |      1 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-23-17\n",
      "  done: false\n",
      "  episode_len_mean: 3.748082357690755\n",
      "  episode_reward_max: 2.25\n",
      "  episode_reward_mean: -1.5036536132418248\n",
      "  episode_reward_min: -9.75\n",
      "  episodes_this_iter: 2477\n",
      "  episodes_total: 5034\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 14568.92\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.341519832611084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017999494448304176\n",
      "        policy_loss: -0.04186198487877846\n",
      "        total_loss: 1.5443516969680786\n",
      "        vf_explained_var: 0.21959862112998962\n",
      "        vf_loss: 1.5826138257980347\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3273491859436035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02470376156270504\n",
      "        policy_loss: -0.05213140696287155\n",
      "        total_loss: 1.8393688201904297\n",
      "        vf_explained_var: 0.21068266034126282\n",
      "        vf_loss: 1.8865593671798706\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.336504340171814\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021876005455851555\n",
      "        policy_loss: -0.044941890984773636\n",
      "        total_loss: 1.7350451946258545\n",
      "        vf_explained_var: 0.23128178715705872\n",
      "        vf_loss: 1.7756118774414062\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.3084163665771484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022955242544412613\n",
      "        policy_loss: -0.04640059173107147\n",
      "        total_loss: 1.8285446166992188\n",
      "        vf_explained_var: 0.2406957447528839\n",
      "        vf_loss: 1.8703542947769165\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.337404489517212\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023109259083867073\n",
      "        policy_loss: -0.05114360526204109\n",
      "        total_loss: 1.9669655561447144\n",
      "        vf_explained_var: 0.22264178097248077\n",
      "        vf_loss: 2.0134875774383545\n",
      "    load_time_ms: 1638.728\n",
      "    num_steps_sampled: 18551\n",
      "    num_steps_trained: 18432\n",
      "    sample_time_ms: 34586.286\n",
      "    update_time_ms: 11262.661\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.10169491525424\n",
      "    ram_util_percent: 42.689830508474586\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 5.5\n",
      "    policy_1: 5.6\n",
      "    policy_2: 5.45\n",
      "    policy_3: 5.5\n",
      "    policy_4: 5.5\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.5254138070246266\n",
      "    policy_1: -0.34822365765038354\n",
      "    policy_2: -0.27585789261203064\n",
      "    policy_3: -0.21511909568025844\n",
      "    policy_4: -0.13903916027452567\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.6808503434317652\n",
      "    mean_inference_ms: 6.492060801187577\n",
      "    mean_processing_ms: 2.33160490573504\n",
      "  sm__effective_map_size: 7\n",
      "  sm__episode_len_mean: 3.748082357690755\n",
      "  sm__episode_reward_max: 2.25\n",
      "  sm__episode_reward_mean: -1.5036536132418248\n",
      "  sm__episode_reward_min: -9.75\n",
      "  sm__policy_0_reward_max: 5.5\n",
      "  sm__policy_0_reward_mean: -0.5254138070246266\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 125.67751669883728\n",
      "  time_this_iter_s: 45.531543254852295\n",
      "  time_total_s: 125.67751669883728\n",
      "  timestamp: 1594934597\n",
      "  timesteps_since_restore: 18551\n",
      "  timesteps_this_iter: 9284\n",
      "  timesteps_total: 18551\n",
      "  training_iteration: 2\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 | -1.50365 |          125.678 | 18551 |      2 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-24-01\n",
      "  done: false\n",
      "  episode_len_mean: 4.186684660368871\n",
      "  episode_reward_max: 2.0999999999999996\n",
      "  episode_reward_mean: -1.4703103913630229\n",
      "  episode_reward_min: -9.75\n",
      "  episodes_this_iter: 2223\n",
      "  episodes_total: 7257\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 13154.31\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2790852785110474\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025130026042461395\n",
      "        policy_loss: -0.052846286445856094\n",
      "        total_loss: 1.9425787925720215\n",
      "        vf_explained_var: 0.24454157054424286\n",
      "        vf_loss: 1.9903990030288696\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2641997337341309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024969803169369698\n",
      "        policy_loss: -0.05373319238424301\n",
      "        total_loss: 2.3079726696014404\n",
      "        vf_explained_var: 0.2269624024629593\n",
      "        vf_loss: 2.354214906692505\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2900006771087646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02311931736767292\n",
      "        policy_loss: -0.059036415070295334\n",
      "        total_loss: 2.265514612197876\n",
      "        vf_explained_var: 0.23935362696647644\n",
      "        vf_loss: 2.317614793777466\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.236467957496643\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02893141843378544\n",
      "        policy_loss: -0.055946964770555496\n",
      "        total_loss: 2.303349256515503\n",
      "        vf_explained_var: 0.24653111398220062\n",
      "        vf_loss: 2.350616693496704\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2809076309204102\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02439756877720356\n",
      "        policy_loss: -0.059547390788793564\n",
      "        total_loss: 2.4183719158172607\n",
      "        vf_explained_var: 0.2361803650856018\n",
      "        vf_loss: 2.47059965133667\n",
      "    load_time_ms: 1290.579\n",
      "    num_steps_sampled: 27858\n",
      "    num_steps_trained: 27648\n",
      "    sample_time_ms: 34195.664\n",
      "    update_time_ms: 7524.102\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.92068965517241\n",
      "    ram_util_percent: 42.82586206896552\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 5.55\n",
      "    policy_1: 5.55\n",
      "    policy_2: 5.5\n",
      "    policy_3: 5.6499999999999995\n",
      "    policy_4: 5.55\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.4538461538461539\n",
      "    policy_1: -0.3375168690958165\n",
      "    policy_2: -0.3595591542959964\n",
      "    policy_3: -0.2014844804318489\n",
      "    policy_4: -0.11790373369320742\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.6882564828304265\n",
      "    mean_inference_ms: 6.442894555081741\n",
      "    mean_processing_ms: 2.2768941201000032\n",
      "  sm__effective_map_size: 7\n",
      "  sm__episode_len_mean: 4.186684660368871\n",
      "  sm__episode_reward_max: 2.0999999999999996\n",
      "  sm__episode_reward_mean: -1.4703103913630229\n",
      "  sm__episode_reward_min: -9.75\n",
      "  sm__policy_0_reward_max: 5.55\n",
      "  sm__policy_0_reward_mean: -0.4538461538461539\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 170.10548281669617\n",
      "  time_this_iter_s: 44.42796611785889\n",
      "  time_total_s: 170.10548281669617\n",
      "  timestamp: 1594934641\n",
      "  timesteps_since_restore: 27858\n",
      "  timesteps_this_iter: 9307\n",
      "  timesteps_total: 27858\n",
      "  training_iteration: 3\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 | -1.47031 |          170.105 | 27858 |      3 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-24-45\n",
      "  done: false\n",
      "  episode_len_mean: 4.8559411146161935\n",
      "  episode_reward_max: 2.05\n",
      "  episode_reward_mean: -1.3419821240799161\n",
      "  episode_reward_min: -9.55\n",
      "  episodes_this_iter: 1902\n",
      "  episodes_total: 9159\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 12424.458\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2259438037872314\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02468319796025753\n",
      "        policy_loss: -0.04852496087551117\n",
      "        total_loss: 2.172515630722046\n",
      "        vf_explained_var: 0.26035478711128235\n",
      "        vf_loss: 2.2136354446411133\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.154374361038208\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02383439987897873\n",
      "        policy_loss: -0.05355031415820122\n",
      "        total_loss: 2.7004706859588623\n",
      "        vf_explained_var: 0.2840155363082886\n",
      "        vf_loss: 2.743295669555664\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2261121273040771\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022518660873174667\n",
      "        policy_loss: -0.05758197605609894\n",
      "        total_loss: 2.6876344680786133\n",
      "        vf_explained_var: 0.27430880069732666\n",
      "        vf_loss: 2.7350831031799316\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.159751296043396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02310802787542343\n",
      "        policy_loss: -0.05833997204899788\n",
      "        total_loss: 2.507012367248535\n",
      "        vf_explained_var: 0.2954113483428955\n",
      "        vf_loss: 2.5549535751342773\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.2207136154174805\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022729257121682167\n",
      "        policy_loss: -0.05477964133024216\n",
      "        total_loss: 3.052072048187256\n",
      "        vf_explained_var: 0.24948962032794952\n",
      "        vf_loss: 3.096623420715332\n",
      "    load_time_ms: 1126.905\n",
      "    num_steps_sampled: 37094\n",
      "    num_steps_trained: 36864\n",
      "    sample_time_ms: 33757.526\n",
      "    update_time_ms: 5647.822\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.58421052631577\n",
      "    ram_util_percent: 42.891228070175444\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 5.6499999999999995\n",
      "    policy_1: 5.6\n",
      "    policy_2: 5.8\n",
      "    policy_3: 5.55\n",
      "    policy_4: 5.6\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.6133017875920084\n",
      "    policy_1: -0.20068349106203995\n",
      "    policy_2: -0.25670347003154576\n",
      "    policy_3: -0.15867507886435334\n",
      "    policy_4: -0.11261829652996848\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.6961729266526573\n",
      "    mean_inference_ms: 6.408749862347844\n",
      "    mean_processing_ms: 2.2011295434933906\n",
      "  sm__effective_map_size: 7\n",
      "  sm__episode_len_mean: 4.8559411146161935\n",
      "  sm__episode_reward_max: 2.05\n",
      "  sm__episode_reward_mean: -1.3419821240799161\n",
      "  sm__episode_reward_min: -9.55\n",
      "  sm__policy_0_reward_max: 5.6499999999999995\n",
      "  sm__policy_0_reward_mean: -0.6133017875920084\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 213.48204159736633\n",
      "  time_this_iter_s: 43.376558780670166\n",
      "  time_total_s: 213.48204159736633\n",
      "  timestamp: 1594934685\n",
      "  timesteps_since_restore: 37094\n",
      "  timesteps_this_iter: 9236\n",
      "  timesteps_total: 37094\n",
      "  training_iteration: 4\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 | -1.34198 |          213.482 | 37094 |      4 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-25-28\n",
      "  done: false\n",
      "  episode_len_mean: 5.51779359430605\n",
      "  episode_reward_max: 2.45\n",
      "  episode_reward_mean: -1.1394128113879005\n",
      "  episode_reward_min: -9.65\n",
      "  episodes_this_iter: 1686\n",
      "  episodes_total: 10845\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 11996.033\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1338558197021484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019353432580828667\n",
      "        policy_loss: -0.04951297864317894\n",
      "        total_loss: 2.8047127723693848\n",
      "        vf_explained_var: 0.30981793999671936\n",
      "        vf_loss: 2.8455166816711426\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1123307943344116\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017877191305160522\n",
      "        policy_loss: -0.04881206899881363\n",
      "        total_loss: 3.290144920349121\n",
      "        vf_explained_var: 0.3049579858779907\n",
      "        vf_loss: 3.326889991760254\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1791198253631592\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01678730547428131\n",
      "        policy_loss: -0.04695111885666847\n",
      "        total_loss: 3.0773110389709473\n",
      "        vf_explained_var: 0.3116007447242737\n",
      "        vf_loss: 3.1129305362701416\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.09459388256073\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01988944783806801\n",
      "        policy_loss: -0.05152615159749985\n",
      "        total_loss: 3.081678628921509\n",
      "        vf_explained_var: 0.3190985918045044\n",
      "        vf_loss: 3.119779348373413\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.170149326324463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01710265502333641\n",
      "        policy_loss: -0.04628952592611313\n",
      "        total_loss: 3.640630006790161\n",
      "        vf_explained_var: 0.27366822957992554\n",
      "        vf_loss: 3.6753756999969482\n",
      "    load_time_ms: 1029.399\n",
      "    num_steps_sampled: 46397\n",
      "    num_steps_trained: 46080\n",
      "    sample_time_ms: 33439.486\n",
      "    update_time_ms: 4522.592\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.83508771929826\n",
      "    ram_util_percent: 43.11754385964912\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 5.9\n",
      "    policy_1: 5.75\n",
      "    policy_2: 5.8\n",
      "    policy_3: 5.8\n",
      "    policy_4: 5.8\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.3552491103202847\n",
      "    policy_1: -0.15901542111506525\n",
      "    policy_2: -0.2680901542111507\n",
      "    policy_3: -0.13440094899169636\n",
      "    policy_4: -0.22265717674970345\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7019197297377584\n",
      "    mean_inference_ms: 6.384255597160656\n",
      "    mean_processing_ms: 2.1340072379416695\n",
      "  sm__effective_map_size: 7\n",
      "  sm__episode_len_mean: 5.51779359430605\n",
      "  sm__episode_reward_max: 2.45\n",
      "  sm__episode_reward_mean: -1.1394128113879005\n",
      "  sm__episode_reward_min: -9.65\n",
      "  sm__policy_0_reward_max: 5.9\n",
      "  sm__policy_0_reward_mean: -0.3552491103202847\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 256.6391370296478\n",
      "  time_this_iter_s: 43.157095432281494\n",
      "  time_total_s: 256.6391370296478\n",
      "  timestamp: 1594934728\n",
      "  timesteps_since_restore: 46397\n",
      "  timesteps_this_iter: 9303\n",
      "  timesteps_total: 46397\n",
      "  training_iteration: 5\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 | -1.13941 |          256.639 | 46397 |      5 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-26-11\n",
      "  done: false\n",
      "  episode_len_mean: 6.382434301521439\n",
      "  episode_reward_max: 3.1000000000000005\n",
      "  episode_reward_mean: -0.9187067773167359\n",
      "  episode_reward_min: -9.75\n",
      "  episodes_this_iter: 1446\n",
      "  episodes_total: 12291\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 11724.611\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.073108196258545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021152475848793983\n",
      "        policy_loss: -0.04647110402584076\n",
      "        total_loss: 3.4153318405151367\n",
      "        vf_explained_var: 0.352887898683548\n",
      "        vf_loss: 3.452284574508667\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0479066371917725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015706883743405342\n",
      "        policy_loss: -0.04410609230399132\n",
      "        total_loss: 3.801943063735962\n",
      "        vf_explained_var: 0.35816702246665955\n",
      "        vf_loss: 3.83544659614563\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1043211221694946\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01838475838303566\n",
      "        policy_loss: -0.050768233835697174\n",
      "        total_loss: 3.77256441116333\n",
      "        vf_explained_var: 0.3380250036716461\n",
      "        vf_loss: 3.8109233379364014\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.009997010231018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01805923879146576\n",
      "        policy_loss: -0.05209873244166374\n",
      "        total_loss: 3.845067024230957\n",
      "        vf_explained_var: 0.3562218248844147\n",
      "        vf_loss: 3.884976387023926\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.1328366994857788\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015299992635846138\n",
      "        policy_loss: -0.04579152539372444\n",
      "        total_loss: 3.737797975540161\n",
      "        vf_explained_var: 0.327032208442688\n",
      "        vf_loss: 3.773261547088623\n",
      "    load_time_ms: 954.113\n",
      "    num_steps_sampled: 55626\n",
      "    num_steps_trained: 55296\n",
      "    sample_time_ms: 33108.505\n",
      "    update_time_ms: 3773.118\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.2\n",
      "    ram_util_percent: 43.1890909090909\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 5.8\n",
      "    policy_1: 5.95\n",
      "    policy_2: 5.9\n",
      "    policy_3: 6.0\n",
      "    policy_4: 5.85\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.41438450899031815\n",
      "    policy_1: -0.026348547717842334\n",
      "    policy_2: -0.3073651452282158\n",
      "    policy_3: -0.006777316735823\n",
      "    policy_4: -0.16383125864453663\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.708410879630771\n",
      "    mean_inference_ms: 6.361154383336573\n",
      "    mean_processing_ms: 2.0677541591155624\n",
      "  sm__effective_map_size: 7\n",
      "  sm__episode_len_mean: 6.382434301521439\n",
      "  sm__episode_reward_max: 3.1000000000000005\n",
      "  sm__episode_reward_mean: -0.9187067773167359\n",
      "  sm__episode_reward_min: -9.75\n",
      "  sm__policy_0_reward_max: 5.8\n",
      "  sm__policy_0_reward_mean: -0.41438450899031815\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 299.1021361351013\n",
      "  time_this_iter_s: 42.46299910545349\n",
      "  time_total_s: 299.1021361351013\n",
      "  timestamp: 1594934771\n",
      "  timesteps_since_restore: 55626\n",
      "  timesteps_this_iter: 9229\n",
      "  timesteps_total: 55626\n",
      "  training_iteration: 6\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+-----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |    reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+-----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 | -0.918707 |          299.102 | 55626 |      6 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+-----------+------------------+-------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-26-53\n",
      "  done: false\n",
      "  episode_len_mean: 7.333070866141732\n",
      "  episode_reward_max: 2.950000000000001\n",
      "  episode_reward_mean: -0.5043700787401575\n",
      "  episode_reward_min: -9.5\n",
      "  episodes_this_iter: 1270\n",
      "  episodes_total: 13561\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 11525.447\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0376118421554565\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014383362606167793\n",
      "        policy_loss: -0.040278494358062744\n",
      "        total_loss: 4.110340595245361\n",
      "        vf_explained_var: 0.39022940397262573\n",
      "        vf_loss: 4.140910625457764\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9958384037017822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01626966893672943\n",
      "        policy_loss: -0.042256176471710205\n",
      "        total_loss: 4.644667625427246\n",
      "        vf_explained_var: 0.39117157459259033\n",
      "        vf_loss: 4.6759419441223145\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0531280040740967\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017357371747493744\n",
      "        policy_loss: -0.04533444344997406\n",
      "        total_loss: 4.829339027404785\n",
      "        vf_explained_var: 0.38025107979774475\n",
      "        vf_loss: 4.862956523895264\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9299294948577881\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01632542349398136\n",
      "        policy_loss: -0.046010542660951614\n",
      "        total_loss: 4.854499816894531\n",
      "        vf_explained_var: 0.3830120861530304\n",
      "        vf_loss: 4.889490604400635\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0816973447799683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017497019842267036\n",
      "        policy_loss: -0.04722418263554573\n",
      "        total_loss: 4.275806427001953\n",
      "        vf_explained_var: 0.3694148063659668\n",
      "        vf_loss: 4.311220169067383\n",
      "    load_time_ms: 912.747\n",
      "    num_steps_sampled: 64939\n",
      "    num_steps_trained: 64512\n",
      "    sample_time_ms: 32879.944\n",
      "    update_time_ms: 3236.641\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.16909090909093\n",
      "    ram_util_percent: 43.16000000000002\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 6.1\n",
      "    policy_1: 5.85\n",
      "    policy_2: 6.2\n",
      "    policy_3: 6.2\n",
      "    policy_4: 6.2\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.38059055118110235\n",
      "    policy_1: 0.06598425196850388\n",
      "    policy_2: -0.1536220472440945\n",
      "    policy_3: 0.1040551181102362\n",
      "    policy_4: -0.1401968503937008\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7145210967084465\n",
      "    mean_inference_ms: 6.339912727514255\n",
      "    mean_processing_ms: 2.007117700714827\n",
      "  sm__effective_map_size: 7\n",
      "  sm__episode_len_mean: 7.333070866141732\n",
      "  sm__episode_reward_max: 2.950000000000001\n",
      "  sm__episode_reward_mean: -0.5043700787401575\n",
      "  sm__episode_reward_min: -9.5\n",
      "  sm__policy_0_reward_max: 6.1\n",
      "  sm__policy_0_reward_mean: -0.38059055118110235\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 341.6612765789032\n",
      "  time_this_iter_s: 42.55914044380188\n",
      "  time_total_s: 341.6612765789032\n",
      "  timestamp: 1594934813\n",
      "  timesteps_since_restore: 64939\n",
      "  timesteps_this_iter: 9313\n",
      "  timesteps_total: 64939\n",
      "  training_iteration: 7\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 | -0.50437 |          341.661 | 64939 |      7 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-27-36\n",
      "  done: false\n",
      "  episode_len_mean: 8.63932898415657\n",
      "  episode_reward_max: 2.950000000000001\n",
      "  episode_reward_mean: -0.19920782851817329\n",
      "  episode_reward_min: -9.0\n",
      "  episodes_this_iter: 1073\n",
      "  episodes_total: 14634\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 11373.576\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.967411994934082\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016751006245613098\n",
      "        policy_loss: -0.03736807405948639\n",
      "        total_loss: 4.6821699142456055\n",
      "        vf_explained_var: 0.42836979031562805\n",
      "        vf_loss: 4.708231449127197\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9365869164466858\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016186565160751343\n",
      "        policy_loss: -0.04133724048733711\n",
      "        total_loss: 6.333095073699951\n",
      "        vf_explained_var: 0.4039541482925415\n",
      "        vf_loss: 6.36350679397583\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0016573667526245\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01755000650882721\n",
      "        policy_loss: -0.04133643954992294\n",
      "        total_loss: 6.084105014801025\n",
      "        vf_explained_var: 0.4236769378185272\n",
      "        vf_loss: 6.113595008850098\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8704400062561035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014843061566352844\n",
      "        policy_loss: -0.038839150220155716\n",
      "        total_loss: 6.8005218505859375\n",
      "        vf_explained_var: 0.40578752756118774\n",
      "        vf_loss: 6.829341888427734\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0432538986206055\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016267871484160423\n",
      "        policy_loss: -0.04371024668216705\n",
      "        total_loss: 5.496711730957031\n",
      "        vf_explained_var: 0.3883129358291626\n",
      "        vf_loss: 5.5294413566589355\n",
      "    load_time_ms: 878.375\n",
      "    num_steps_sampled: 74209\n",
      "    num_steps_trained: 73728\n",
      "    sample_time_ms: 32644.981\n",
      "    update_time_ms: 2835.393\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.64727272727274\n",
      "    ram_util_percent: 43.22909090909092\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 6.4\n",
      "    policy_1: 6.3500000000000005\n",
      "    policy_2: 6.300000000000001\n",
      "    policy_3: 6.4\n",
      "    policy_4: 6.0\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.1733457595526561\n",
      "    policy_1: -0.019524697110904002\n",
      "    policy_2: 0.023112767940354138\n",
      "    policy_3: 0.23536812674743704\n",
      "    policy_4: -0.2648182665424045\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7183120055388499\n",
      "    mean_inference_ms: 6.323296873074225\n",
      "    mean_processing_ms: 1.9495695197431675\n",
      "  sm__effective_map_size: 7\n",
      "  sm__episode_len_mean: 8.63932898415657\n",
      "  sm__episode_reward_max: 2.950000000000001\n",
      "  sm__episode_reward_mean: -0.19920782851817329\n",
      "  sm__episode_reward_min: -9.0\n",
      "  sm__policy_0_reward_max: 6.4\n",
      "  sm__policy_0_reward_mean: -0.1733457595526561\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 383.6701557636261\n",
      "  time_this_iter_s: 42.0088791847229\n",
      "  time_total_s: 383.6701557636261\n",
      "  timestamp: 1594934856\n",
      "  timesteps_since_restore: 74209\n",
      "  timesteps_this_iter: 9270\n",
      "  timesteps_total: 74209\n",
      "  training_iteration: 8\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+-----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |    reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+-----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 | -0.199208 |           383.67 | 74209 |      8 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+-----------+------------------+-------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-28-18\n",
      "  done: false\n",
      "  episode_len_mean: 10.431767337807607\n",
      "  episode_reward_max: 4.049999999999997\n",
      "  episode_reward_mean: 0.26957494407158844\n",
      "  episode_reward_min: -8.5\n",
      "  episodes_this_iter: 894\n",
      "  episodes_total: 15528\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 11257.559\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9635699391365051\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013871499337255955\n",
      "        policy_loss: -0.03375045582652092\n",
      "        total_loss: 5.560018062591553\n",
      "        vf_explained_var: 0.4760642647743225\n",
      "        vf_loss: 5.584405422210693\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.872277557849884\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015133870765566826\n",
      "        policy_loss: -0.03670589625835419\n",
      "        total_loss: 8.468257904052734\n",
      "        vf_explained_var: 0.4538459777832031\n",
      "        vf_loss: 8.494749069213867\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9464292526245117\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014580775983631611\n",
      "        policy_loss: -0.03868385776877403\n",
      "        total_loss: 7.756441116333008\n",
      "        vf_explained_var: 0.45972946286201477\n",
      "        vf_loss: 7.785284042358398\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7787940502166748\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012606787495315075\n",
      "        policy_loss: -0.033504024147987366\n",
      "        total_loss: 8.869902610778809\n",
      "        vf_explained_var: 0.4521988332271576\n",
      "        vf_loss: 8.894899368286133\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0289770364761353\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013326473534107208\n",
      "        policy_loss: -0.039801888167858124\n",
      "        total_loss: 6.0683064460754395\n",
      "        vf_explained_var: 0.4437220096588135\n",
      "        vf_loss: 6.0991129875183105\n",
      "    load_time_ms: 850.089\n",
      "    num_steps_sampled: 83535\n",
      "    num_steps_trained: 82944\n",
      "    sample_time_ms: 32465.13\n",
      "    update_time_ms: 2522.439\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.61636363636364\n",
      "    ram_util_percent: 43.39454545454546\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 6.3500000000000005\n",
      "    policy_1: 7.000000000000001\n",
      "    policy_2: 6.300000000000001\n",
      "    policy_3: 6.700000000000001\n",
      "    policy_4: 6.6000000000000005\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.39932885906040266\n",
      "    policy_1: 0.04496644295302016\n",
      "    policy_2: 0.11034675615212529\n",
      "    policy_3: 0.7153803131991051\n",
      "    policy_4: -0.2017897091722595\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7228445184336556\n",
      "    mean_inference_ms: 6.311714026223802\n",
      "    mean_processing_ms: 1.8894145898875403\n",
      "  sm__effective_map_size: 7\n",
      "  sm__episode_len_mean: 10.431767337807607\n",
      "  sm__episode_reward_max: 4.049999999999997\n",
      "  sm__episode_reward_mean: 0.26957494407158844\n",
      "  sm__episode_reward_min: -8.5\n",
      "  sm__policy_0_reward_max: 6.3500000000000005\n",
      "  sm__policy_0_reward_mean: -0.39932885906040266\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 425.70077633857727\n",
      "  time_this_iter_s: 42.03062057495117\n",
      "  time_total_s: 425.70077633857727\n",
      "  timestamp: 1594934898\n",
      "  timesteps_since_restore: 83535\n",
      "  timesteps_this_iter: 9326\n",
      "  timesteps_total: 83535\n",
      "  training_iteration: 9\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.4/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 | 0.269575 |          425.701 | 83535 |      9 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-29-01\n",
      "  done: false\n",
      "  episode_len_mean: 13.29243937232525\n",
      "  episode_reward_max: 5.549999999999986\n",
      "  episode_reward_mean: 0.7788159771754636\n",
      "  episode_reward_min: -8.25\n",
      "  episodes_this_iter: 701\n",
      "  episodes_total: 16229\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 11173.836\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9273703098297119\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010631438344717026\n",
      "        policy_loss: -0.03340252861380577\n",
      "        total_loss: 7.196388244628906\n",
      "        vf_explained_var: 0.5234967470169067\n",
      "        vf_loss: 7.22261381149292\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8444918990135193\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012286284938454628\n",
      "        policy_loss: -0.0290968120098114\n",
      "        total_loss: 11.315868377685547\n",
      "        vf_explained_var: 0.5256583094596863\n",
      "        vf_loss: 11.336671829223633\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9122122526168823\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014713449403643608\n",
      "        policy_loss: -0.03571655601263046\n",
      "        total_loss: 11.80453872680664\n",
      "        vf_explained_var: 0.5076634883880615\n",
      "        vf_loss: 11.83032512664795\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.701219916343689\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012010947801172733\n",
      "        policy_loss: -0.03054087422788143\n",
      "        total_loss: 14.532703399658203\n",
      "        vf_explained_var: 0.4764847159385681\n",
      "        vf_loss: 14.555136680603027\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 1.0066251754760742\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016986183822155\n",
      "        policy_loss: -0.035827212035655975\n",
      "        total_loss: 7.238653182983398\n",
      "        vf_explained_var: 0.4968847632408142\n",
      "        vf_loss: 7.26301383972168\n",
      "    load_time_ms: 832.198\n",
      "    num_steps_sampled: 92853\n",
      "    num_steps_trained: 92160\n",
      "    sample_time_ms: 32395.224\n",
      "    update_time_ms: 2272.068\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.19107142857142\n",
      "    ram_util_percent: 43.55714285714286\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 6.800000000000001\n",
      "    policy_1: 7.449999999999999\n",
      "    policy_2: 7.35\n",
      "    policy_3: 8.099999999999998\n",
      "    policy_4: 6.950000000000001\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.30021398002853067\n",
      "    policy_1: 0.20699001426533528\n",
      "    policy_2: 0.19443651925820257\n",
      "    policy_3: 0.915620542082739\n",
      "    policy_4: -0.23801711840228246\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.731670703432792\n",
      "    mean_inference_ms: 6.3339558688578625\n",
      "    mean_processing_ms: 1.8389637087276423\n",
      "  sm__effective_map_size: 7\n",
      "  sm__episode_len_mean: 13.29243937232525\n",
      "  sm__episode_reward_max: 5.549999999999986\n",
      "  sm__episode_reward_mean: 0.7788159771754636\n",
      "  sm__episode_reward_min: -8.25\n",
      "  sm__policy_0_reward_max: 6.800000000000001\n",
      "  sm__policy_0_reward_mean: -0.30021398002853067\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 468.6044673919678\n",
      "  time_this_iter_s: 42.9036910533905\n",
      "  time_total_s: 468.6044673919678\n",
      "  timestamp: 1594934941\n",
      "  timesteps_since_restore: 92853\n",
      "  timesteps_this_iter: 9318\n",
      "  timesteps_total: 92853\n",
      "  training_iteration: 10\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.4/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |    ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+-------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 | 0.778816 |          468.604 | 92853 |     10 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+-------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-29-43\n",
      "  done: false\n",
      "  episode_len_mean: 15.717171717171718\n",
      "  episode_reward_max: 7.5499999999999865\n",
      "  episode_reward_mean: 1.187205387205387\n",
      "  episode_reward_min: -6.85\n",
      "  episodes_this_iter: 594\n",
      "  episodes_total: 16823\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10342.28\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9197733402252197\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014100968837738037\n",
      "        policy_loss: -0.0229500699788332\n",
      "        total_loss: 8.37462043762207\n",
      "        vf_explained_var: 0.5519576668739319\n",
      "        vf_loss: 8.388052940368652\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.887007474899292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016115538775920868\n",
      "        policy_loss: -0.026733310893177986\n",
      "        total_loss: 12.805065155029297\n",
      "        vf_explained_var: 0.571367084980011\n",
      "        vf_loss: 12.820920944213867\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8975708484649658\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01399172842502594\n",
      "        policy_loss: -0.03362516686320305\n",
      "        total_loss: 13.678037643432617\n",
      "        vf_explained_var: 0.5468382239341736\n",
      "        vf_loss: 13.70221996307373\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6229389309883118\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011127112433314323\n",
      "        policy_loss: -0.026812097057700157\n",
      "        total_loss: 18.875377655029297\n",
      "        vf_explained_var: 0.5105943083763123\n",
      "        vf_loss: 18.894676208496094\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.9364078044891357\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0174321997910738\n",
      "        policy_loss: -0.036040160804986954\n",
      "        total_loss: 11.00394058227539\n",
      "        vf_explained_var: 0.5297996997833252\n",
      "        vf_loss: 11.028214454650879\n",
      "    load_time_ms: 629.402\n",
      "    num_steps_sampled: 102189\n",
      "    num_steps_trained: 101376\n",
      "    sample_time_ms: 32022.704\n",
      "    update_time_ms: 23.821\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.65454545454546\n",
      "    ram_util_percent: 43.60181818181818\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 7.35\n",
      "    policy_1: 7.35\n",
      "    policy_2: 7.050000000000001\n",
      "    policy_3: 8.499999999999996\n",
      "    policy_4: 6.950000000000001\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.2777777777777778\n",
      "    policy_1: -0.018518518518518507\n",
      "    policy_2: -0.031902356902356876\n",
      "    policy_3: 1.5494949494949497\n",
      "    policy_4: -0.0340909090909091\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7332862264179258\n",
      "    mean_inference_ms: 6.343717643838171\n",
      "    mean_processing_ms: 1.7870143140943677\n",
      "  sm__effective_map_size: 9\n",
      "  sm__episode_len_mean: 15.717171717171718\n",
      "  sm__episode_reward_max: 7.5499999999999865\n",
      "  sm__episode_reward_mean: 1.187205387205387\n",
      "  sm__episode_reward_min: -6.85\n",
      "  sm__policy_0_reward_max: 7.35\n",
      "  sm__policy_0_reward_mean: -0.2777777777777778\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 510.71437430381775\n",
      "  time_this_iter_s: 42.109906911849976\n",
      "  time_total_s: 510.71437430381775\n",
      "  timestamp: 1594934983\n",
      "  timesteps_since_restore: 102189\n",
      "  timesteps_this_iter: 9336\n",
      "  timesteps_total: 102189\n",
      "  training_iteration: 11\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.4/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  1.18721 |          510.714 | 102189 |     11 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 14.099547511312217\n",
      "  episode_reward_max: 5.149999999999995\n",
      "  episode_reward_mean: 0.8270739064856709\n",
      "  episode_reward_min: -7.9\n",
      "  episodes_this_iter: 663\n",
      "  episodes_total: 17486\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10354.807\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8958932161331177\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011049339547753334\n",
      "        policy_loss: -0.02916673570871353\n",
      "        total_loss: 10.629154205322266\n",
      "        vf_explained_var: 0.5167029500007629\n",
      "        vf_loss: 10.650861740112305\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8117117881774902\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014723754487931728\n",
      "        policy_loss: -0.029500285163521767\n",
      "        total_loss: 15.712034225463867\n",
      "        vf_explained_var: 0.5285428762435913\n",
      "        vf_loss: 15.73159408569336\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8364461660385132\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015682131052017212\n",
      "        policy_loss: -0.03348447382450104\n",
      "        total_loss: 18.125141143798828\n",
      "        vf_explained_var: 0.4545190632343292\n",
      "        vf_loss: 18.14803695678711\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7242157459259033\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012930085882544518\n",
      "        policy_loss: -0.03130745515227318\n",
      "        total_loss: 23.58184242248535\n",
      "        vf_explained_var: 0.46284690499305725\n",
      "        vf_loss: 23.60442352294922\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7872995734214783\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013067325577139854\n",
      "        policy_loss: -0.03535447269678116\n",
      "        total_loss: 11.12553596496582\n",
      "        vf_explained_var: 0.4663747549057007\n",
      "        vf_loss: 11.152070999145508\n",
      "    load_time_ms: 628.616\n",
      "    num_steps_sampled: 111537\n",
      "    num_steps_trained: 110592\n",
      "    sample_time_ms: 31648.078\n",
      "    update_time_ms: 23.481\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.79444444444445\n",
      "    ram_util_percent: 43.64074074074073\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 6.700000000000001\n",
      "    policy_1: 7.699999999999998\n",
      "    policy_2: 7.1000000000000005\n",
      "    policy_3: 7.699999999999998\n",
      "    policy_4: 7.050000000000001\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.19343891402714933\n",
      "    policy_1: 0.2904223227752639\n",
      "    policy_2: 0.4721719457013574\n",
      "    policy_3: 0.570814479638009\n",
      "    policy_4: -0.31289592760180995\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7346708565958704\n",
      "    mean_inference_ms: 6.339603154612882\n",
      "    mean_processing_ms: 1.7477862154717807\n",
      "  sm__effective_map_size: 9\n",
      "  sm__episode_len_mean: 14.099547511312217\n",
      "  sm__episode_reward_max: 5.149999999999995\n",
      "  sm__episode_reward_mean: 0.8270739064856709\n",
      "  sm__episode_reward_min: -7.9\n",
      "  sm__policy_0_reward_max: 6.700000000000001\n",
      "  sm__policy_0_reward_mean: -0.19343891402714933\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 552.5926012992859\n",
      "  time_this_iter_s: 41.87822699546814\n",
      "  time_total_s: 552.5926012992859\n",
      "  timestamp: 1594935025\n",
      "  timesteps_since_restore: 111537\n",
      "  timesteps_this_iter: 9348\n",
      "  timesteps_total: 111537\n",
      "  training_iteration: 12\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.4/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 | 0.827074 |          552.593 | 111537 |     12 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-31-07\n",
      "  done: false\n",
      "  episode_len_mean: 21.029411764705884\n",
      "  episode_reward_max: 10.95000000000002\n",
      "  episode_reward_mean: 1.7106334841628947\n",
      "  episode_reward_min: -7.35\n",
      "  episodes_this_iter: 442\n",
      "  episodes_total: 17928\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10355.126\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8626079559326172\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009604892693459988\n",
      "        policy_loss: -0.027101585641503334\n",
      "        total_loss: 11.52253532409668\n",
      "        vf_explained_var: 0.638640820980072\n",
      "        vf_loss: 11.543152809143066\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7441050410270691\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01110015343874693\n",
      "        policy_loss: -0.028567126020789146\n",
      "        total_loss: 19.48035430908203\n",
      "        vf_explained_var: 0.6159624457359314\n",
      "        vf_loss: 19.50143051147461\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8340660333633423\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013913248665630817\n",
      "        policy_loss: -0.03070637956261635\n",
      "        total_loss: 22.16260528564453\n",
      "        vf_explained_var: 0.6031358242034912\n",
      "        vf_loss: 22.183921813964844\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.699400782585144\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011095348745584488\n",
      "        policy_loss: -0.023193921893835068\n",
      "        total_loss: 25.797603607177734\n",
      "        vf_explained_var: 0.6047095656394958\n",
      "        vf_loss: 25.813308715820312\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7825916409492493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0117371566593647\n",
      "        policy_loss: -0.031129973009228706\n",
      "        total_loss: 11.887819290161133\n",
      "        vf_explained_var: 0.5753060579299927\n",
      "        vf_loss: 11.911026000976562\n",
      "    load_time_ms: 633.293\n",
      "    num_steps_sampled: 120832\n",
      "    num_steps_trained: 119808\n",
      "    sample_time_ms: 31381.743\n",
      "    update_time_ms: 20.782\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.72962962962963\n",
      "    ram_util_percent: 43.60185185185184\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 7.599999999999999\n",
      "    policy_1: 8.999999999999993\n",
      "    policy_2: 9.649999999999991\n",
      "    policy_3: 10.29999999999999\n",
      "    policy_4: 7.3999999999999995\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.07002262443438911\n",
      "    policy_1: 0.3785067873303168\n",
      "    policy_2: 0.553393665158371\n",
      "    policy_3: 0.9630090497737556\n",
      "    policy_4: -0.11425339366515835\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7355088764451747\n",
      "    mean_inference_ms: 6.344916412238491\n",
      "    mean_processing_ms: 1.704583853827376\n",
      "  sm__effective_map_size: 9\n",
      "  sm__episode_len_mean: 21.029411764705884\n",
      "  sm__episode_reward_max: 10.95000000000002\n",
      "  sm__episode_reward_mean: 1.7106334841628947\n",
      "  sm__episode_reward_min: -7.35\n",
      "  sm__policy_0_reward_max: 7.599999999999999\n",
      "  sm__policy_0_reward_mean: -0.07002262443438911\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 594.3578534126282\n",
      "  time_this_iter_s: 41.765252113342285\n",
      "  time_total_s: 594.3578534126282\n",
      "  timestamp: 1594935067\n",
      "  timesteps_since_restore: 120832\n",
      "  timesteps_this_iter: 9295\n",
      "  timesteps_total: 120832\n",
      "  training_iteration: 13\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.4/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  1.71063 |          594.358 | 120832 |     13 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-31-48\n",
      "  done: false\n",
      "  episode_len_mean: 26.213483146067414\n",
      "  episode_reward_max: 13.400000000000041\n",
      "  episode_reward_mean: 2.567977528089885\n",
      "  episode_reward_min: -1.75\n",
      "  episodes_this_iter: 356\n",
      "  episodes_total: 18284\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10378.734\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8424747586250305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007956977933645248\n",
      "        policy_loss: -0.02095990628004074\n",
      "        total_loss: 14.818283081054688\n",
      "        vf_explained_var: 0.695953905582428\n",
      "        vf_loss: 14.833873748779297\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7374107241630554\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010444190353155136\n",
      "        policy_loss: -0.02470666915178299\n",
      "        total_loss: 22.523706436157227\n",
      "        vf_explained_var: 0.679797887802124\n",
      "        vf_loss: 22.54136085510254\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.815292239189148\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011496912688016891\n",
      "        policy_loss: -0.026934795081615448\n",
      "        total_loss: 25.91771697998047\n",
      "        vf_explained_var: 0.6578457355499268\n",
      "        vf_loss: 25.936893463134766\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6925010681152344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009823629632592201\n",
      "        policy_loss: -0.02321414090692997\n",
      "        total_loss: 30.869720458984375\n",
      "        vf_explained_var: 0.6539309024810791\n",
      "        vf_loss: 30.886301040649414\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7267646789550781\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008829694241285324\n",
      "        policy_loss: -0.027048125863075256\n",
      "        total_loss: 15.736884117126465\n",
      "        vf_explained_var: 0.6486663818359375\n",
      "        vf_loss: 15.75797176361084\n",
      "    load_time_ms: 629.415\n",
      "    num_steps_sampled: 130164\n",
      "    num_steps_trained: 129024\n",
      "    sample_time_ms: 31155.999\n",
      "    update_time_ms: 21.385\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.48333333333332\n",
      "    ram_util_percent: 43.80740740740742\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 9.499999999999993\n",
      "    policy_1: 8.849999999999994\n",
      "    policy_2: 8.899999999999995\n",
      "    policy_3: 10.949999999999987\n",
      "    policy_4: 8.849999999999994\n",
      "  policy_reward_mean:\n",
      "    policy_0: 0.08398876404494375\n",
      "    policy_1: 0.4476123595505617\n",
      "    policy_2: 0.6089887640449437\n",
      "    policy_3: 1.0487359550561797\n",
      "    policy_4: 0.37865168539325844\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7339275559853626\n",
      "    mean_inference_ms: 6.34814674517461\n",
      "    mean_processing_ms: 1.6635309103669185\n",
      "  sm__effective_map_size: 9\n",
      "  sm__episode_len_mean: 26.213483146067414\n",
      "  sm__episode_reward_max: 13.400000000000041\n",
      "  sm__episode_reward_mean: 2.567977528089885\n",
      "  sm__episode_reward_min: -1.75\n",
      "  sm__policy_0_reward_max: 9.499999999999993\n",
      "  sm__policy_0_reward_mean: 0.08398876404494375\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 635.6610124111176\n",
      "  time_this_iter_s: 41.30315899848938\n",
      "  time_total_s: 635.6610124111176\n",
      "  timestamp: 1594935108\n",
      "  timesteps_since_restore: 130164\n",
      "  timesteps_this_iter: 9332\n",
      "  timesteps_total: 130164\n",
      "  training_iteration: 14\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.4/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  2.56798 |          635.661 | 130164 |     14 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-32-30\n",
      "  done: false\n",
      "  episode_len_mean: 34.544444444444444\n",
      "  episode_reward_max: 13.650000000000045\n",
      "  episode_reward_mean: 3.6461111111111073\n",
      "  episode_reward_min: -7.6\n",
      "  episodes_this_iter: 270\n",
      "  episodes_total: 18554\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10396.952\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8676279783248901\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01045103557407856\n",
      "        policy_loss: -0.023105589672923088\n",
      "        total_loss: 16.848661422729492\n",
      "        vf_explained_var: 0.7345759272575378\n",
      "        vf_loss: 16.86471176147461\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7198416590690613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01029158290475607\n",
      "        policy_loss: -0.021206233650445938\n",
      "        total_loss: 25.682640075683594\n",
      "        vf_explained_var: 0.7469986081123352\n",
      "        vf_loss: 25.6968994140625\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8154251575469971\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01036855112761259\n",
      "        policy_loss: -0.02508080191910267\n",
      "        total_loss: 28.72312355041504\n",
      "        vf_explained_var: 0.7056562900543213\n",
      "        vf_loss: 28.741207122802734\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6637030243873596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008123531937599182\n",
      "        policy_loss: -0.01771589368581772\n",
      "        total_loss: 34.91796112060547\n",
      "        vf_explained_var: 0.7196905016899109\n",
      "        vf_loss: 34.930198669433594\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6756159067153931\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010105947963893414\n",
      "        policy_loss: -0.024103594943881035\n",
      "        total_loss: 21.297279357910156\n",
      "        vf_explained_var: 0.7089035511016846\n",
      "        vf_loss: 21.314559936523438\n",
      "    load_time_ms: 629.833\n",
      "    num_steps_sampled: 139491\n",
      "    num_steps_trained: 138240\n",
      "    sample_time_ms: 30938.783\n",
      "    update_time_ms: 21.02\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.7685185185185\n",
      "    ram_util_percent: 43.87222222222225\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 9.299999999999994\n",
      "    policy_1: 9.99999999999999\n",
      "    policy_2: 9.99999999999999\n",
      "    policy_3: 9.99999999999999\n",
      "    policy_4: 8.599999999999994\n",
      "  policy_reward_mean:\n",
      "    policy_0: 0.13444444444444434\n",
      "    policy_1: 0.67537037037037\n",
      "    policy_2: 0.7661111111111109\n",
      "    policy_3: 1.5651851851851843\n",
      "    policy_4: 0.5049999999999999\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7333181371369721\n",
      "    mean_inference_ms: 6.346716573634681\n",
      "    mean_processing_ms: 1.6242564137156887\n",
      "  sm__effective_map_size: 9\n",
      "  sm__episode_len_mean: 34.544444444444444\n",
      "  sm__episode_reward_max: 13.650000000000045\n",
      "  sm__episode_reward_mean: 3.6461111111111073\n",
      "  sm__episode_reward_min: -7.6\n",
      "  sm__policy_0_reward_max: 9.299999999999994\n",
      "  sm__policy_0_reward_mean: 0.13444444444444434\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 676.9860579967499\n",
      "  time_this_iter_s: 41.325045585632324\n",
      "  time_total_s: 676.9860579967499\n",
      "  timestamp: 1594935150\n",
      "  timesteps_since_restore: 139491\n",
      "  timesteps_this_iter: 9327\n",
      "  timesteps_total: 139491\n",
      "  training_iteration: 15\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.4/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  3.64611 |          676.986 | 139491 |     15 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-33-11\n",
      "  done: false\n",
      "  episode_len_mean: 43.43255813953488\n",
      "  episode_reward_max: 16.15000000000008\n",
      "  episode_reward_mean: 4.679767441860462\n",
      "  episode_reward_min: -1.1000000000000085\n",
      "  episodes_this_iter: 215\n",
      "  episodes_total: 18769\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10417.989\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8709126710891724\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009532378055155277\n",
      "        policy_loss: -0.018561633303761482\n",
      "        total_loss: 17.694494247436523\n",
      "        vf_explained_var: 0.8017085790634155\n",
      "        vf_loss: 17.706621170043945\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7223637104034424\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009876817464828491\n",
      "        policy_loss: -0.021814975887537003\n",
      "        total_loss: 31.04136085510254\n",
      "        vf_explained_var: 0.7817263603210449\n",
      "        vf_loss: 31.05651092529297\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8070408701896667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009368069469928741\n",
      "        policy_loss: -0.021457456052303314\n",
      "        total_loss: 32.56329345703125\n",
      "        vf_explained_var: 0.7654432654380798\n",
      "        vf_loss: 32.57843017578125\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6837842464447021\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010242710821330547\n",
      "        policy_loss: -0.020390711724758148\n",
      "        total_loss: 38.808753967285156\n",
      "        vf_explained_var: 0.7674745917320251\n",
      "        vf_loss: 38.82223129272461\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.580232560634613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009976167231798172\n",
      "        policy_loss: -0.025546785444021225\n",
      "        total_loss: 28.621768951416016\n",
      "        vf_explained_var: 0.7340096235275269\n",
      "        vf_loss: 28.64058494567871\n",
      "    load_time_ms: 636.805\n",
      "    num_steps_sampled: 148829\n",
      "    num_steps_trained: 147456\n",
      "    sample_time_ms: 30777.557\n",
      "    update_time_ms: 20.59\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.90377358490566\n",
      "    ram_util_percent: 43.96037735849056\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 9.299999999999994\n",
      "    policy_1: 11.499999999999986\n",
      "    policy_2: 11.499999999999986\n",
      "    policy_3: 10.14999999999999\n",
      "    policy_4: 9.699999999999992\n",
      "  policy_reward_mean:\n",
      "    policy_0: -0.05441860465116321\n",
      "    policy_1: 1.11813953488372\n",
      "    policy_2: 1.0081395348837203\n",
      "    policy_3: 1.2676744186046502\n",
      "    policy_4: 1.3402325581395338\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.731725368709849\n",
      "    mean_inference_ms: 6.346944870236996\n",
      "    mean_processing_ms: 1.587226849851369\n",
      "  sm__effective_map_size: 9\n",
      "  sm__episode_len_mean: 43.43255813953488\n",
      "  sm__episode_reward_max: 16.15000000000008\n",
      "  sm__episode_reward_mean: 4.679767441860462\n",
      "  sm__episode_reward_min: -1.1000000000000085\n",
      "  sm__policy_0_reward_max: 9.299999999999994\n",
      "  sm__policy_0_reward_mean: -0.05441860465116321\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 718.0964226722717\n",
      "  time_this_iter_s: 41.11036467552185\n",
      "  time_total_s: 718.0964226722717\n",
      "  timestamp: 1594935191\n",
      "  timesteps_since_restore: 148829\n",
      "  timesteps_this_iter: 9338\n",
      "  timesteps_total: 148829\n",
      "  training_iteration: 16\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  4.67977 |          718.096 | 148829 |     16 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-33-51\n",
      "  done: false\n",
      "  episode_len_mean: 55.437869822485204\n",
      "  episode_reward_max: 18.850000000000133\n",
      "  episode_reward_mean: 6.476923076923076\n",
      "  episode_reward_min: -0.25\n",
      "  episodes_this_iter: 169\n",
      "  episodes_total: 18938\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10414.699\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.768615186214447\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008995873853564262\n",
      "        policy_loss: -0.013113236986100674\n",
      "        total_loss: 27.655908584594727\n",
      "        vf_explained_var: 0.8339433670043945\n",
      "        vf_loss: 27.662954330444336\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.66426020860672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0077062081545591354\n",
      "        policy_loss: -0.019116397947072983\n",
      "        total_loss: 36.99506378173828\n",
      "        vf_explained_var: 0.8090354204177856\n",
      "        vf_loss: 37.00897979736328\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.842919647693634\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012662186287343502\n",
      "        policy_loss: -0.020462190732359886\n",
      "        total_loss: 35.66260528564453\n",
      "        vf_explained_var: 0.7986072301864624\n",
      "        vf_loss: 35.67451477050781\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6805506348609924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007929553277790546\n",
      "        policy_loss: -0.018551956862211227\n",
      "        total_loss: 40.804325103759766\n",
      "        vf_explained_var: 0.8045427799224854\n",
      "        vf_loss: 40.81752395629883\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.59365314245224\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009014349430799484\n",
      "        policy_loss: -0.01543255615979433\n",
      "        total_loss: 34.995792388916016\n",
      "        vf_explained_var: 0.7786498069763184\n",
      "        vf_loss: 35.00514221191406\n",
      "    load_time_ms: 634.499\n",
      "    num_steps_sampled: 158198\n",
      "    num_steps_trained: 156672\n",
      "    sample_time_ms: 30590.082\n",
      "    update_time_ms: 20.998\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.71698113207547\n",
      "    ram_util_percent: 44.12830188679243\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 11.849999999999984\n",
      "    policy_1: 11.349999999999985\n",
      "    policy_2: 8.699999999999996\n",
      "    policy_3: 11.599999999999984\n",
      "    policy_4: 9.99999999999999\n",
      "  policy_reward_mean:\n",
      "    policy_0: 0.7023668639053243\n",
      "    policy_1: 1.2180473372781049\n",
      "    policy_2: 0.8801775147928984\n",
      "    policy_3: 2.181656804733726\n",
      "    policy_4: 1.4946745562130161\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7317049948769758\n",
      "    mean_inference_ms: 6.345585187030455\n",
      "    mean_processing_ms: 1.5519178527121684\n",
      "  sm__effective_map_size: 9\n",
      "  sm__episode_len_mean: 55.437869822485204\n",
      "  sm__episode_reward_max: 18.850000000000133\n",
      "  sm__episode_reward_mean: 6.476923076923076\n",
      "  sm__episode_reward_min: -0.25\n",
      "  sm__policy_0_reward_max: 11.849999999999984\n",
      "  sm__policy_0_reward_mean: 0.7023668639053243\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 758.7134108543396\n",
      "  time_this_iter_s: 40.61698818206787\n",
      "  time_total_s: 758.7134108543396\n",
      "  timestamp: 1594935231\n",
      "  timesteps_since_restore: 158198\n",
      "  timesteps_this_iter: 9369\n",
      "  timesteps_total: 158198\n",
      "  training_iteration: 17\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  6.47692 |          758.713 | 158198 |     17 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-34-32\n",
      "  done: false\n",
      "  episode_len_mean: 58.78616352201258\n",
      "  episode_reward_max: 19.60000000000013\n",
      "  episode_reward_mean: 7.133962264150946\n",
      "  episode_reward_min: -0.4500000000000002\n",
      "  episodes_this_iter: 159\n",
      "  episodes_total: 19097\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10422.51\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8186542987823486\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007952268235385418\n",
      "        policy_loss: -0.013599900528788567\n",
      "        total_loss: 32.67927169799805\n",
      "        vf_explained_var: 0.8337600231170654\n",
      "        vf_loss: 32.687503814697266\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6299331784248352\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007231913506984711\n",
      "        policy_loss: -0.013472477905452251\n",
      "        total_loss: 40.85420608520508\n",
      "        vf_explained_var: 0.8236240148544312\n",
      "        vf_loss: 40.86279296875\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8391937613487244\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008062709122896194\n",
      "        policy_loss: -0.01859125681221485\n",
      "        total_loss: 35.156593322753906\n",
      "        vf_explained_var: 0.8174693584442139\n",
      "        vf_loss: 35.16974639892578\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6696560382843018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007863095961511135\n",
      "        policy_loss: -0.01663830503821373\n",
      "        total_loss: 45.27836227416992\n",
      "        vf_explained_var: 0.8247855305671692\n",
      "        vf_loss: 45.28969192504883\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.5490061044692993\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007394315674901009\n",
      "        policy_loss: -0.01650664396584034\n",
      "        total_loss: 42.36581039428711\n",
      "        vf_explained_var: 0.8024503588676453\n",
      "        vf_loss: 42.37732696533203\n",
      "    load_time_ms: 629.957\n",
      "    num_steps_sampled: 167545\n",
      "    num_steps_trained: 165888\n",
      "    sample_time_ms: 30453.896\n",
      "    update_time_ms: 20.873\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.76415094339623\n",
      "    ram_util_percent: 44.3509433962264\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 10.549999999999988\n",
      "    policy_1: 12.64999999999998\n",
      "    policy_2: 10.49999999999999\n",
      "    policy_3: 10.699999999999989\n",
      "    policy_4: 9.99999999999999\n",
      "  policy_reward_mean:\n",
      "    policy_0: 0.399056603773584\n",
      "    policy_1: 2.377987421383646\n",
      "    policy_2: 0.8474842767295587\n",
      "    policy_3: 1.8232704402515707\n",
      "    policy_4: 1.6861635220125766\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7301561725395052\n",
      "    mean_inference_ms: 6.345497537958223\n",
      "    mean_processing_ms: 1.5202558503249328\n",
      "  sm__effective_map_size: 9\n",
      "  sm__episode_len_mean: 58.78616352201258\n",
      "  sm__episode_reward_max: 19.60000000000013\n",
      "  sm__episode_reward_mean: 7.133962264150946\n",
      "  sm__episode_reward_min: -0.4500000000000002\n",
      "  sm__policy_0_reward_max: 10.549999999999988\n",
      "  sm__policy_0_reward_mean: 0.399056603773584\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 799.3816757202148\n",
      "  time_this_iter_s: 40.668264865875244\n",
      "  time_total_s: 799.3816757202148\n",
      "  timestamp: 1594935272\n",
      "  timesteps_since_restore: 167545\n",
      "  timesteps_this_iter: 9347\n",
      "  timesteps_total: 167545\n",
      "  training_iteration: 18\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  7.13396 |          799.382 | 167545 |     18 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-35-12\n",
      "  done: false\n",
      "  episode_len_mean: 72.03100775193798\n",
      "  episode_reward_max: 26.75000000000023\n",
      "  episode_reward_mean: 8.464728682170556\n",
      "  episode_reward_min: -7.6\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 19226\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10417.72\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7582287192344666\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0056397276930511\n",
      "        policy_loss: -0.015391498804092407\n",
      "        total_loss: 38.81207275390625\n",
      "        vf_explained_var: 0.8624367713928223\n",
      "        vf_loss: 38.82365798950195\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.5958865880966187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007145977579057217\n",
      "        policy_loss: -0.01591113582253456\n",
      "        total_loss: 42.016815185546875\n",
      "        vf_explained_var: 0.8621551394462585\n",
      "        vf_loss: 42.02790069580078\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8505376577377319\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009976815432310104\n",
      "        policy_loss: -0.015402182936668396\n",
      "        total_loss: 33.94541931152344\n",
      "        vf_explained_var: 0.8429199457168579\n",
      "        vf_loss: 33.95408630371094\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7245968580245972\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008897596038877964\n",
      "        policy_loss: -0.015460861846804619\n",
      "        total_loss: 48.08030700683594\n",
      "        vf_explained_var: 0.8524076342582703\n",
      "        vf_loss: 48.08976745605469\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.5276709198951721\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00895463302731514\n",
      "        policy_loss: -0.016371913254261017\n",
      "        total_loss: 49.84477233886719\n",
      "        vf_explained_var: 0.823678195476532\n",
      "        vf_loss: 49.8550910949707\n",
      "    load_time_ms: 631.119\n",
      "    num_steps_sampled: 176837\n",
      "    num_steps_trained: 175104\n",
      "    sample_time_ms: 30257.66\n",
      "    update_time_ms: 21.106\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.11923076923077\n",
      "    ram_util_percent: 44.486538461538466\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 14.750000000000004\n",
      "    policy_1: 13.04999999999998\n",
      "    policy_2: 10.199999999999989\n",
      "    policy_3: 11.949999999999983\n",
      "    policy_4: 13.699999999999989\n",
      "  policy_reward_mean:\n",
      "    policy_0: 1.2406976744186025\n",
      "    policy_1: 2.2674418604651128\n",
      "    policy_2: 1.3166666666666653\n",
      "    policy_3: 1.6620155038759665\n",
      "    policy_4: 1.9779069767441833\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.730251841696287\n",
      "    mean_inference_ms: 6.3381861909708155\n",
      "    mean_processing_ms: 1.4914817484994245\n",
      "  sm__effective_map_size: 9\n",
      "  sm__episode_len_mean: 72.03100775193798\n",
      "  sm__episode_reward_max: 26.75000000000023\n",
      "  sm__episode_reward_mean: 8.464728682170556\n",
      "  sm__episode_reward_min: -7.6\n",
      "  sm__policy_0_reward_max: 14.750000000000004\n",
      "  sm__policy_0_reward_mean: 1.2406976744186025\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 839.4052925109863\n",
      "  time_this_iter_s: 40.023616790771484\n",
      "  time_total_s: 839.4052925109863\n",
      "  timestamp: 1594935312\n",
      "  timesteps_since_restore: 176837\n",
      "  timesteps_this_iter: 9292\n",
      "  timesteps_total: 176837\n",
      "  training_iteration: 19\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  8.46473 |          839.405 | 176837 |     19 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-35-53\n",
      "  done: false\n",
      "  episode_len_mean: 72.36434108527132\n",
      "  episode_reward_max: 25.400000000000226\n",
      "  episode_reward_mean: 8.8356589147287\n",
      "  episode_reward_min: 0.7000000000000002\n",
      "  episodes_this_iter: 129\n",
      "  episodes_total: 19355\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10420.261\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8303779363632202\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006389666348695755\n",
      "        policy_loss: -0.01331762783229351\n",
      "        total_loss: 35.85108947753906\n",
      "        vf_explained_var: 0.8801181316375732\n",
      "        vf_loss: 35.86009216308594\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.599680483341217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006851126905530691\n",
      "        policy_loss: -0.01819811761379242\n",
      "        total_loss: 56.6229362487793\n",
      "        vf_explained_var: 0.856680691242218\n",
      "        vf_loss: 56.63650894165039\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.8291130661964417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008801307529211044\n",
      "        policy_loss: -0.016663609072566032\n",
      "        total_loss: 44.232303619384766\n",
      "        vf_explained_var: 0.8378474712371826\n",
      "        vf_loss: 44.24302673339844\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6478054523468018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007410299498587847\n",
      "        policy_loss: -0.016627881675958633\n",
      "        total_loss: 57.52541732788086\n",
      "        vf_explained_var: 0.8493229150772095\n",
      "        vf_loss: 57.53704071044922\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.4346804618835449\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006850844249129295\n",
      "        policy_loss: -0.017028439790010452\n",
      "        total_loss: 60.99037551879883\n",
      "        vf_explained_var: 0.8211244940757751\n",
      "        vf_loss: 61.00277328491211\n",
      "    load_time_ms: 630.326\n",
      "    num_steps_sampled: 186172\n",
      "    num_steps_trained: 184320\n",
      "    sample_time_ms: 30031.381\n",
      "    update_time_ms: 21.156\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.48301886792453\n",
      "    ram_util_percent: 44.46792452830191\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 13.74999999999999\n",
      "    policy_1: 12.349999999999982\n",
      "    policy_2: 9.99999999999999\n",
      "    policy_3: 12.999999999999979\n",
      "    policy_4: 12.59999999999998\n",
      "  policy_reward_mean:\n",
      "    policy_0: 0.9383720930232542\n",
      "    policy_1: 2.3585271317829424\n",
      "    policy_2: 1.4790697674418591\n",
      "    policy_3: 2.034496124031005\n",
      "    policy_4: 2.0251937984496085\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.730095495098658\n",
      "    mean_inference_ms: 6.3320113516654395\n",
      "    mean_processing_ms: 1.4651286560440955\n",
      "  sm__effective_map_size: 9\n",
      "  sm__episode_len_mean: 72.36434108527132\n",
      "  sm__episode_reward_max: 25.400000000000226\n",
      "  sm__episode_reward_mean: 8.8356589147287\n",
      "  sm__episode_reward_min: 0.7000000000000002\n",
      "  sm__policy_0_reward_max: 13.74999999999999\n",
      "  sm__policy_0_reward_mean: 0.9383720930232542\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 880.0587553977966\n",
      "  time_this_iter_s: 40.6534628868103\n",
      "  time_total_s: 880.0587553977966\n",
      "  timestamp: 1594935353\n",
      "  timesteps_since_restore: 186172\n",
      "  timesteps_this_iter: 9335\n",
      "  timesteps_total: 186172\n",
      "  training_iteration: 20\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 6.5/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  8.83566 |          880.059 | 186172 |     20 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-36-35\n",
      "  done: false\n",
      "  episode_len_mean: 79.76470588235294\n",
      "  episode_reward_max: 23.700000000000188\n",
      "  episode_reward_mean: 9.616806722689093\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 19474\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10463.977\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7920531034469604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007214798126369715\n",
      "        policy_loss: -0.013008290901780128\n",
      "        total_loss: 42.09139633178711\n",
      "        vf_explained_var: 0.8908751010894775\n",
      "        vf_loss: 42.09953689575195\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6166015267372131\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005193670280277729\n",
      "        policy_loss: -0.01188425812870264\n",
      "        total_loss: 56.758609771728516\n",
      "        vf_explained_var: 0.8787317872047424\n",
      "        vf_loss: 56.766990661621094\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7634017467498779\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009471263736486435\n",
      "        policy_loss: -0.015379918739199638\n",
      "        total_loss: 45.88031005859375\n",
      "        vf_explained_var: 0.8661372065544128\n",
      "        vf_loss: 45.88929748535156\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6544256806373596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007313231937587261\n",
      "        policy_loss: -0.01347406767308712\n",
      "        total_loss: 66.0328369140625\n",
      "        vf_explained_var: 0.8622226119041443\n",
      "        vf_loss: 66.04136657714844\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.5257266759872437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007592601701617241\n",
      "        policy_loss: -0.014510968700051308\n",
      "        total_loss: 59.75566101074219\n",
      "        vf_explained_var: 0.8681600689888\n",
      "        vf_loss: 59.765045166015625\n",
      "    load_time_ms: 636.616\n",
      "    num_steps_sampled: 195664\n",
      "    num_steps_trained: 193792\n",
      "    sample_time_ms: 29943.277\n",
      "    update_time_ms: 21.061\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.52592592592593\n",
      "    ram_util_percent: 45.45185185185187\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 15.750000000000018\n",
      "    policy_1: 15.100000000000009\n",
      "    policy_2: 12.999999999999979\n",
      "    policy_3: 11.749999999999984\n",
      "    policy_4: 10.799999999999986\n",
      "  policy_reward_mean:\n",
      "    policy_0: 0.8995798319327715\n",
      "    policy_1: 2.608823529411762\n",
      "    policy_2: 1.571428571428569\n",
      "    policy_3: 2.3016806722689047\n",
      "    policy_4: 2.2352941176470553\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7306893179779315\n",
      "    mean_inference_ms: 6.328652536923499\n",
      "    mean_processing_ms: 1.4414588009847014\n",
      "  sm__effective_map_size: 11\n",
      "  sm__episode_len_mean: 79.76470588235294\n",
      "  sm__episode_reward_max: 23.700000000000188\n",
      "  sm__episode_reward_mean: 9.616806722689093\n",
      "  sm__episode_reward_min: 0.0\n",
      "  sm__policy_0_reward_max: 15.750000000000018\n",
      "  sm__policy_0_reward_mean: 0.8995798319327715\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 921.7793760299683\n",
      "  time_this_iter_s: 41.72062063217163\n",
      "  time_total_s: 921.7793760299683\n",
      "  timestamp: 1594935395\n",
      "  timesteps_since_restore: 195664\n",
      "  timesteps_this_iter: 9492\n",
      "  timesteps_total: 195664\n",
      "  training_iteration: 21\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 7.0/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  9.61681 |          921.779 | 195664 |     21 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-37-17\n",
      "  done: false\n",
      "  episode_len_mean: 51.36756756756757\n",
      "  episode_reward_max: 24.050000000000193\n",
      "  episode_reward_mean: 5.398918918918919\n",
      "  episode_reward_min: -2.6000000000000005\n",
      "  episodes_this_iter: 185\n",
      "  episodes_total: 19659\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10483.899\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.5263407230377197\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005408211145550013\n",
      "        policy_loss: -0.013989780098199844\n",
      "        total_loss: 55.546669006347656\n",
      "        vf_explained_var: 0.8285323977470398\n",
      "        vf_loss: 55.557010650634766\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.37036287784576416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006726454012095928\n",
      "        policy_loss: -0.014249238185584545\n",
      "        total_loss: 101.98347473144531\n",
      "        vf_explained_var: 0.7233920693397522\n",
      "        vf_loss: 101.9931869506836\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.4471292197704315\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013705198653042316\n",
      "        policy_loss: -0.014191062189638615\n",
      "        total_loss: 26.23866844177246\n",
      "        vf_explained_var: 0.4931391179561615\n",
      "        vf_loss: 26.243606567382812\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7254636883735657\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010353468358516693\n",
      "        policy_loss: -0.01828976720571518\n",
      "        total_loss: 79.1081314086914\n",
      "        vf_explained_var: 0.7542305588722229\n",
      "        vf_loss: 79.11943817138672\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.5480104684829712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016262585297226906\n",
      "        policy_loss: -0.02043517306447029\n",
      "        total_loss: 63.22444534301758\n",
      "        vf_explained_var: 0.6991577744483948\n",
      "        vf_loss: 63.2338981628418\n",
      "    load_time_ms: 643.023\n",
      "    num_steps_sampled: 205167\n",
      "    num_steps_trained: 203264\n",
      "    sample_time_ms: 29924.472\n",
      "    update_time_ms: 21.378\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.3777777777778\n",
      "    ram_util_percent: 47.70185185185183\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 11.049999999999986\n",
      "    policy_1: 14.5\n",
      "    policy_2: 10.199999999999989\n",
      "    policy_3: 10.449999999999989\n",
      "    policy_4: 9.99999999999999\n",
      "  policy_reward_mean:\n",
      "    policy_0: 1.747837837837836\n",
      "    policy_1: 1.948378378378376\n",
      "    policy_2: 0.5999999999999992\n",
      "    policy_3: 1.4843243243243232\n",
      "    policy_4: -0.38162162162162205\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7306129227899947\n",
      "    mean_inference_ms: 6.327512529053916\n",
      "    mean_processing_ms: 1.4213576392056586\n",
      "  sm__effective_map_size: 11\n",
      "  sm__episode_len_mean: 51.36756756756757\n",
      "  sm__episode_reward_max: 24.050000000000193\n",
      "  sm__episode_reward_mean: 5.398918918918919\n",
      "  sm__episode_reward_min: -2.6000000000000005\n",
      "  sm__policy_0_reward_max: 11.049999999999986\n",
      "  sm__policy_0_reward_mean: 1.747837837837836\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 963.7293231487274\n",
      "  time_this_iter_s: 41.949947118759155\n",
      "  time_total_s: 963.7293231487274\n",
      "  timestamp: 1594935437\n",
      "  timesteps_since_restore: 205167\n",
      "  timesteps_this_iter: 9503\n",
      "  timesteps_total: 205167\n",
      "  training_iteration: 22\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 7.0/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  5.39892 |          963.729 | 205167 |     22 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-37-57\n",
      "  done: false\n",
      "  episode_len_mean: 67.22463768115942\n",
      "  episode_reward_max: 19.55000000000013\n",
      "  episode_reward_mean: 7.395652173913051\n",
      "  episode_reward_min: -1.0999999999999996\n",
      "  episodes_this_iter: 138\n",
      "  episodes_total: 19797\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10505.552\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.5907785892486572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006486945785582066\n",
      "        policy_loss: -0.01418398693203926\n",
      "        total_loss: 41.57774353027344\n",
      "        vf_explained_var: 0.8829691410064697\n",
      "        vf_loss: 41.58755111694336\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.43759438395500183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005781218409538269\n",
      "        policy_loss: -0.01157466322183609\n",
      "        total_loss: 83.29143524169922\n",
      "        vf_explained_var: 0.818519651889801\n",
      "        vf_loss: 83.29910278320312\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.49575915932655334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01480330340564251\n",
      "        policy_loss: -0.01174505241215229\n",
      "        total_loss: 22.875913619995117\n",
      "        vf_explained_var: 0.6692140698432922\n",
      "        vf_loss: 22.87766456604004\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7112561464309692\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011951624415814877\n",
      "        policy_loss: -0.015228092670440674\n",
      "        total_loss: 67.46652221679688\n",
      "        vf_explained_var: 0.8027316927909851\n",
      "        vf_loss: 67.47367095947266\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.47630152106285095\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020452139899134636\n",
      "        policy_loss: -0.015015234239399433\n",
      "        total_loss: 54.68758773803711\n",
      "        vf_explained_var: 0.7790187001228333\n",
      "        vf_loss: 54.68879699707031\n",
      "    load_time_ms: 646.018\n",
      "    num_steps_sampled: 214444\n",
      "    num_steps_trained: 212480\n",
      "    sample_time_ms: 29776.297\n",
      "    update_time_ms: 21.323\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.84615384615384\n",
      "    ram_util_percent: 48.17692307692309\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 12.94999999999998\n",
      "    policy_1: 11.449999999999985\n",
      "    policy_2: 13.14999999999998\n",
      "    policy_3: 11.099999999999987\n",
      "    policy_4: 9.99999999999999\n",
      "  policy_reward_mean:\n",
      "    policy_0: 1.2974637681159396\n",
      "    policy_1: 2.2518115942028962\n",
      "    policy_2: 1.5652173913043461\n",
      "    policy_3: 1.3101449275362298\n",
      "    policy_4: 0.9710144927536213\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7319721478500756\n",
      "    mean_inference_ms: 6.322992858523299\n",
      "    mean_processing_ms: 1.4020361507851609\n",
      "  sm__effective_map_size: 11\n",
      "  sm__episode_len_mean: 67.22463768115942\n",
      "  sm__episode_reward_max: 19.55000000000013\n",
      "  sm__episode_reward_mean: 7.395652173913051\n",
      "  sm__episode_reward_min: -1.0999999999999996\n",
      "  sm__policy_0_reward_max: 12.94999999999998\n",
      "  sm__policy_0_reward_mean: 1.2974637681159396\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 1004.2552771568298\n",
      "  time_this_iter_s: 40.52595400810242\n",
      "  time_total_s: 1004.2552771568298\n",
      "  timestamp: 1594935477\n",
      "  timesteps_since_restore: 214444\n",
      "  timesteps_this_iter: 9277\n",
      "  timesteps_total: 214444\n",
      "  training_iteration: 23\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 7.2/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  7.39565 |          1004.26 | 214444 |     23 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-38-39\n",
      "  done: false\n",
      "  episode_len_mean: 75.25984251968504\n",
      "  episode_reward_max: 25.25000000000021\n",
      "  episode_reward_mean: 8.418897637795286\n",
      "  episode_reward_min: -0.20000000000000018\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 19924\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10536.049\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.5352717041969299\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006840194575488567\n",
      "        policy_loss: -0.013530354015529156\n",
      "        total_loss: 51.761959075927734\n",
      "        vf_explained_var: 0.8942326903343201\n",
      "        vf_loss: 51.770870208740234\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.4840400815010071\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005277919117361307\n",
      "        policy_loss: -0.012880233116447926\n",
      "        total_loss: 76.12513732910156\n",
      "        vf_explained_var: 0.8637773990631104\n",
      "        vf_loss: 76.13446044921875\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.43498337268829346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009066298604011536\n",
      "        policy_loss: -0.014120498672127724\n",
      "        total_loss: 27.366132736206055\n",
      "        vf_explained_var: 0.7832202315330505\n",
      "        vf_loss: 27.374135971069336\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.7144069671630859\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007032961584627628\n",
      "        policy_loss: -0.012804804369807243\n",
      "        total_loss: 64.82443237304688\n",
      "        vf_explained_var: 0.8332473039627075\n",
      "        vf_loss: 64.8324966430664\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.4605661928653717\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00829336978495121\n",
      "        policy_loss: -0.016274934634566307\n",
      "        total_loss: 57.622840881347656\n",
      "        vf_explained_var: 0.8219837546348572\n",
      "        vf_loss: 57.63072204589844\n",
      "    load_time_ms: 655.921\n",
      "    num_steps_sampled: 224002\n",
      "    num_steps_trained: 221952\n",
      "    sample_time_ms: 29797.933\n",
      "    update_time_ms: 21.434\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.78490566037736\n",
      "    ram_util_percent: 49.179245283018886\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 14.45\n",
      "    policy_1: 12.699999999999982\n",
      "    policy_2: 11.099999999999987\n",
      "    policy_3: 10.699999999999989\n",
      "    policy_4: 9.99999999999999\n",
      "  policy_reward_mean:\n",
      "    policy_0: 1.8602362204724383\n",
      "    policy_1: 1.659448818897635\n",
      "    policy_2: 2.1015748031496035\n",
      "    policy_3: 1.7850393700787381\n",
      "    policy_4: 1.012598425196848\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7340408568569181\n",
      "    mean_inference_ms: 6.32181547071256\n",
      "    mean_processing_ms: 1.3834225897166303\n",
      "  sm__effective_map_size: 11\n",
      "  sm__episode_len_mean: 75.25984251968504\n",
      "  sm__episode_reward_max: 25.25000000000021\n",
      "  sm__episode_reward_mean: 8.418897637795286\n",
      "  sm__episode_reward_min: -0.20000000000000018\n",
      "  sm__policy_0_reward_max: 14.45\n",
      "  sm__policy_0_reward_mean: 1.8602362204724383\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 1046.1766347885132\n",
      "  time_this_iter_s: 41.92135763168335\n",
      "  time_total_s: 1046.1766347885132\n",
      "  timestamp: 1594935519\n",
      "  timesteps_since_restore: 224002\n",
      "  timesteps_this_iter: 9558\n",
      "  timesteps_total: 224002\n",
      "  training_iteration: 24\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 7.2/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |   8.4189 |          1046.18 | 224002 |     24 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-39-20\n",
      "  done: false\n",
      "  episode_len_mean: 77.65546218487395\n",
      "  episode_reward_max: 24.550000000000203\n",
      "  episode_reward_mean: 8.878571428571446\n",
      "  episode_reward_min: -0.7000000000000002\n",
      "  episodes_this_iter: 119\n",
      "  episodes_total: 20043\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10549.96\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.5897325873374939\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004818500950932503\n",
      "        policy_loss: -0.012401007115840912\n",
      "        total_loss: 54.9111213684082\n",
      "        vf_explained_var: 0.8834401369094849\n",
      "        vf_loss: 54.92027282714844\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.3934507966041565\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005444810260087252\n",
      "        policy_loss: -0.012188809923827648\n",
      "        total_loss: 73.63810729980469\n",
      "        vf_explained_var: 0.879227340221405\n",
      "        vf_loss: 73.6466293334961\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.46133893728256226\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0073479595594108105\n",
      "        policy_loss: -0.012863305397331715\n",
      "        total_loss: 32.59181213378906\n",
      "        vf_explained_var: 0.830158531665802\n",
      "        vf_loss: 32.59971618652344\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6644574403762817\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009579411707818508\n",
      "        policy_loss: -0.01313716545701027\n",
      "        total_loss: 60.8238525390625\n",
      "        vf_explained_var: 0.8542520403862\n",
      "        vf_loss: 60.83053970336914\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.40493452548980713\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006033166311681271\n",
      "        policy_loss: -0.016255712136626244\n",
      "        total_loss: 62.80733871459961\n",
      "        vf_explained_var: 0.8427873849868774\n",
      "        vf_loss: 62.81748580932617\n",
      "    load_time_ms: 661.394\n",
      "    num_steps_sampled: 233243\n",
      "    num_steps_trained: 231168\n",
      "    sample_time_ms: 29753.473\n",
      "    update_time_ms: 21.674\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.6096153846154\n",
      "    ram_util_percent: 49.18269230769231\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 10.349999999999989\n",
      "    policy_1: 14.199999999999996\n",
      "    policy_2: 14.299999999999997\n",
      "    policy_3: 11.399999999999984\n",
      "    policy_4: 10.699999999999989\n",
      "  policy_reward_mean:\n",
      "    policy_0: 1.4184873949579804\n",
      "    policy_1: 2.4647058823529377\n",
      "    policy_2: 1.6054621848739479\n",
      "    policy_3: 1.8499999999999974\n",
      "    policy_4: 1.5399159663865516\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.8\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7362339603070063\n",
      "    mean_inference_ms: 6.3270026809990965\n",
      "    mean_processing_ms: 1.3675509295388975\n",
      "  sm__effective_map_size: 11\n",
      "  sm__episode_len_mean: 77.65546218487395\n",
      "  sm__episode_reward_max: 24.550000000000203\n",
      "  sm__episode_reward_mean: 8.878571428571446\n",
      "  sm__episode_reward_min: -0.7000000000000002\n",
      "  sm__policy_0_reward_max: 10.349999999999989\n",
      "  sm__policy_0_reward_mean: 1.4184873949579804\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 1087.0709764957428\n",
      "  time_this_iter_s: 40.894341707229614\n",
      "  time_total_s: 1087.0709764957428\n",
      "  timestamp: 1594935560\n",
      "  timesteps_since_restore: 233243\n",
      "  timesteps_this_iter: 9241\n",
      "  timesteps_total: 233243\n",
      "  training_iteration: 25\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 7.2/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  8.87857 |          1087.07 | 233243 |     25 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-40-01\n",
      "  done: false\n",
      "  episode_len_mean: 88.68571428571428\n",
      "  episode_reward_max: 26.500000000000227\n",
      "  episode_reward_mean: 10.693333333333362\n",
      "  episode_reward_min: 0.4500000000000002\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 20148\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10540.003\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.64852374792099\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008418659679591656\n",
      "        policy_loss: -0.01378058735281229\n",
      "        total_loss: 43.863800048828125\n",
      "        vf_explained_var: 0.9030368328094482\n",
      "        vf_loss: 43.87474060058594\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.4114105701446533\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004473031032830477\n",
      "        policy_loss: -0.010471774265170097\n",
      "        total_loss: 87.7605972290039\n",
      "        vf_explained_var: 0.8886527419090271\n",
      "        vf_loss: 87.7680435180664\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.45160743594169617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0079190107062459\n",
      "        policy_loss: -0.010558318346738815\n",
      "        total_loss: 40.55933380126953\n",
      "        vf_explained_var: 0.8714784383773804\n",
      "        vf_loss: 40.56454086303711\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.5693787932395935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00863248948007822\n",
      "        policy_loss: -0.014747384935617447\n",
      "        total_loss: 52.682395935058594\n",
      "        vf_explained_var: 0.8935721516609192\n",
      "        vf_loss: 52.69131088256836\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.38786444067955017\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006051539909094572\n",
      "        policy_loss: -0.012601320631802082\n",
      "        total_loss: 59.58473205566406\n",
      "        vf_explained_var: 0.8736047744750977\n",
      "        vf_loss: 59.59120559692383\n",
      "    load_time_ms: 660.811\n",
      "    num_steps_sampled: 242555\n",
      "    num_steps_trained: 240384\n",
      "    sample_time_ms: 29719.776\n",
      "    update_time_ms: 21.681\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.85098039215687\n",
      "    ram_util_percent: 49.24705882352942\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 16.300000000000026\n",
      "    policy_1: 11.749999999999984\n",
      "    policy_2: 14.149999999999995\n",
      "    policy_3: 11.149999999999986\n",
      "    policy_4: 11.349999999999985\n",
      "  policy_reward_mean:\n",
      "    policy_0: 1.4999999999999978\n",
      "    policy_1: 2.7490476190476163\n",
      "    policy_2: 2.4261904761904733\n",
      "    policy_3: 2.1347619047619006\n",
      "    policy_4: 1.8833333333333295\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7378692736083237\n",
      "    mean_inference_ms: 6.3257327679211075\n",
      "    mean_processing_ms: 1.3510354193757805\n",
      "  sm__effective_map_size: 11\n",
      "  sm__episode_len_mean: 88.68571428571428\n",
      "  sm__episode_reward_max: 26.500000000000227\n",
      "  sm__episode_reward_mean: 10.693333333333362\n",
      "  sm__episode_reward_min: 0.4500000000000002\n",
      "  sm__policy_0_reward_max: 16.300000000000026\n",
      "  sm__policy_0_reward_mean: 1.4999999999999978\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 1127.7379705905914\n",
      "  time_this_iter_s: 40.66699409484863\n",
      "  time_total_s: 1127.7379705905914\n",
      "  timestamp: 1594935601\n",
      "  timesteps_since_restore: 242555\n",
      "  timesteps_this_iter: 9312\n",
      "  timesteps_total: 242555\n",
      "  training_iteration: 26\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 7.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  10.6933 |          1127.74 | 242555 |     26 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-40-42\n",
      "  done: false\n",
      "  episode_len_mean: 88.88571428571429\n",
      "  episode_reward_max: 25.450000000000212\n",
      "  episode_reward_mean: 10.48095238095241\n",
      "  episode_reward_min: 0.40000000000000036\n",
      "  episodes_this_iter: 105\n",
      "  episodes_total: 20253\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10555.217\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.565355122089386\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009580093435943127\n",
      "        policy_loss: -0.009972444735467434\n",
      "        total_loss: 61.56697082519531\n",
      "        vf_explained_var: 0.9051649570465088\n",
      "        vf_loss: 61.57370376586914\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.4407542645931244\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007607523817569017\n",
      "        policy_loss: -0.011443328112363815\n",
      "        total_loss: 75.57738494873047\n",
      "        vf_explained_var: 0.8922874927520752\n",
      "        vf_loss: 75.58626556396484\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.45340850949287415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008256015367805958\n",
      "        policy_loss: -0.01183867733925581\n",
      "        total_loss: 46.596012115478516\n",
      "        vf_explained_var: 0.8699132800102234\n",
      "        vf_loss: 46.6022834777832\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6010042428970337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004941002931445837\n",
      "        policy_loss: -0.013853047043085098\n",
      "        total_loss: 62.52534484863281\n",
      "        vf_explained_var: 0.8848945498466492\n",
      "        vf_loss: 62.53586959838867\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.32119375467300415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0058416882529854774\n",
      "        policy_loss: -0.0129267368465662\n",
      "        total_loss: 67.08113861083984\n",
      "        vf_explained_var: 0.8756835460662842\n",
      "        vf_loss: 67.0881576538086\n",
      "    load_time_ms: 657.797\n",
      "    num_steps_sampled: 251888\n",
      "    num_steps_trained: 249600\n",
      "    sample_time_ms: 29701.26\n",
      "    update_time_ms: 21.601\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.48076923076924\n",
      "    ram_util_percent: 49.29230769230767\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 11.299999999999986\n",
      "    policy_1: 13.249999999999982\n",
      "    policy_2: 9.99999999999999\n",
      "    policy_3: 12.549999999999981\n",
      "    policy_4: 11.749999999999984\n",
      "  policy_reward_mean:\n",
      "    policy_0: 1.9738095238095203\n",
      "    policy_1: 2.139047619047615\n",
      "    policy_2: 1.8476190476190448\n",
      "    policy_3: 1.7847619047619017\n",
      "    policy_4: 2.735714285714281\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7401077329441075\n",
      "    mean_inference_ms: 6.322281604961761\n",
      "    mean_processing_ms: 1.335726453335582\n",
      "  sm__effective_map_size: 11\n",
      "  sm__episode_len_mean: 88.88571428571429\n",
      "  sm__episode_reward_max: 25.450000000000212\n",
      "  sm__episode_reward_mean: 10.48095238095241\n",
      "  sm__episode_reward_min: 0.40000000000000036\n",
      "  sm__policy_0_reward_max: 11.299999999999986\n",
      "  sm__policy_0_reward_mean: 1.9738095238095203\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 1168.2900745868683\n",
      "  time_this_iter_s: 40.552103996276855\n",
      "  time_total_s: 1168.2900745868683\n",
      "  timestamp: 1594935642\n",
      "  timesteps_since_restore: 251888\n",
      "  timesteps_this_iter: 9333\n",
      "  timesteps_total: 251888\n",
      "  training_iteration: 27\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 7.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |   10.481 |          1168.29 | 251888 |     27 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-41-23\n",
      "  done: false\n",
      "  episode_len_mean: 95.68\n",
      "  episode_reward_max: 25.70000000000023\n",
      "  episode_reward_mean: 11.500000000000039\n",
      "  episode_reward_min: 0.04999999999999982\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 20351\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10563.886\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6502629518508911\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008623506873846054\n",
      "        policy_loss: -0.010077928192913532\n",
      "        total_loss: 68.04769897460938\n",
      "        vf_explained_var: 0.8951800465583801\n",
      "        vf_loss: 68.05487823486328\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.4346596598625183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007671289145946503\n",
      "        policy_loss: -0.008837675675749779\n",
      "        total_loss: 65.92068481445312\n",
      "        vf_explained_var: 0.9092350006103516\n",
      "        vf_loss: 65.92692565917969\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.4531223475933075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006362724117934704\n",
      "        policy_loss: -0.016104066744446754\n",
      "        total_loss: 49.909385681152344\n",
      "        vf_explained_var: 0.8864880204200745\n",
      "        vf_loss: 49.92119598388672\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6160811185836792\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007948600687086582\n",
      "        policy_loss: -0.01677681691944599\n",
      "        total_loss: 67.20736694335938\n",
      "        vf_explained_var: 0.8965966701507568\n",
      "        vf_loss: 67.22146606445312\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.3202124834060669\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003490055911242962\n",
      "        policy_loss: -0.01636499911546707\n",
      "        total_loss: 66.32220458984375\n",
      "        vf_explained_var: 0.895282506942749\n",
      "        vf_loss: 66.33503723144531\n",
      "    load_time_ms: 661.265\n",
      "    num_steps_sampled: 261279\n",
      "    num_steps_trained: 258816\n",
      "    sample_time_ms: 29723.358\n",
      "    update_time_ms: 21.076\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.80769230769229\n",
      "    ram_util_percent: 49.37692307692309\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 13.649999999999988\n",
      "    policy_1: 16.150000000000023\n",
      "    policy_2: 11.549999999999985\n",
      "    policy_3: 11.599999999999984\n",
      "    policy_4: 11.449999999999985\n",
      "  policy_reward_mean:\n",
      "    policy_0: 1.6134999999999977\n",
      "    policy_1: 2.6309999999999967\n",
      "    policy_2: 2.169999999999997\n",
      "    policy_3: 1.718499999999996\n",
      "    policy_4: 3.3669999999999956\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.95\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7410695545858335\n",
      "    mean_inference_ms: 6.322424563679331\n",
      "    mean_processing_ms: 1.3219631321427128\n",
      "  sm__effective_map_size: 11\n",
      "  sm__episode_len_mean: 95.68\n",
      "  sm__episode_reward_max: 25.70000000000023\n",
      "  sm__episode_reward_mean: 11.500000000000039\n",
      "  sm__episode_reward_min: 0.04999999999999982\n",
      "  sm__policy_0_reward_max: 13.649999999999988\n",
      "  sm__policy_0_reward_mean: 1.6134999999999977\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 1209.2937982082367\n",
      "  time_this_iter_s: 41.00372362136841\n",
      "  time_total_s: 1209.2937982082367\n",
      "  timestamp: 1594935683\n",
      "  timesteps_since_restore: 261279\n",
      "  timesteps_this_iter: 9391\n",
      "  timesteps_total: 261279\n",
      "  training_iteration: 28\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 7.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |     11.5 |          1209.29 | 261279 |     28 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-42-04\n",
      "  done: false\n",
      "  episode_len_mean: 92.64356435643565\n",
      "  episode_reward_max: 22.55000000000017\n",
      "  episode_reward_mean: 11.766336633663405\n",
      "  episode_reward_min: -0.5\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 20452\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10595.883\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6302216053009033\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012014318257570267\n",
      "        policy_loss: -0.009269727393984795\n",
      "        total_loss: 75.7830810546875\n",
      "        vf_explained_var: 0.8942385911941528\n",
      "        vf_loss: 75.78829956054688\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.39160609245300293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0073539516888558865\n",
      "        policy_loss: -0.013331469148397446\n",
      "        total_loss: 85.91080474853516\n",
      "        vf_explained_var: 0.8991732001304626\n",
      "        vf_loss: 85.92166137695312\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.48426833748817444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0073935347609221935\n",
      "        policy_loss: -0.016377465799450874\n",
      "        total_loss: 46.77824401855469\n",
      "        vf_explained_var: 0.8798635005950928\n",
      "        vf_loss: 46.78963851928711\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6115984320640564\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009590805508196354\n",
      "        policy_loss: -0.014841509982943535\n",
      "        total_loss: 73.39561462402344\n",
      "        vf_explained_var: 0.8881108164787292\n",
      "        vf_loss: 73.4072265625\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.3303152322769165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007892600260674953\n",
      "        policy_loss: -0.012034794315695763\n",
      "        total_loss: 67.13092041015625\n",
      "        vf_explained_var: 0.895548939704895\n",
      "        vf_loss: 67.13895416259766\n",
      "    load_time_ms: 671.591\n",
      "    num_steps_sampled: 270636\n",
      "    num_steps_trained: 268032\n",
      "    sample_time_ms: 29824.71\n",
      "    update_time_ms: 21.234\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.05192307692307\n",
      "    ram_util_percent: 49.33269230769229\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 11.749999999999984\n",
      "    policy_1: 13.649999999999988\n",
      "    policy_2: 11.099999999999987\n",
      "    policy_3: 11.999999999999982\n",
      "    policy_4: 11.749999999999984\n",
      "  policy_reward_mean:\n",
      "    policy_0: 1.8108910891089076\n",
      "    policy_1: 3.053465346534648\n",
      "    policy_2: 1.8470297029702947\n",
      "    policy_3: 2.136138613861382\n",
      "    policy_4: 2.9188118811881134\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.8\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.7439656913586763\n",
      "    mean_inference_ms: 6.326730088084935\n",
      "    mean_processing_ms: 1.3076618744435233\n",
      "  sm__effective_map_size: 11\n",
      "  sm__episode_len_mean: 92.64356435643565\n",
      "  sm__episode_reward_max: 22.55000000000017\n",
      "  sm__episode_reward_mean: 11.766336633663405\n",
      "  sm__episode_reward_min: -0.5\n",
      "  sm__policy_0_reward_max: 11.749999999999984\n",
      "  sm__policy_0_reward_mean: 1.8108910891089076\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 1250.7558088302612\n",
      "  time_this_iter_s: 41.462010622024536\n",
      "  time_total_s: 1250.7558088302612\n",
      "  timestamp: 1594935724\n",
      "  timesteps_since_restore: 270636\n",
      "  timesteps_this_iter: 9357\n",
      "  timesteps_total: 270636\n",
      "  training_iteration: 29\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 7.2/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  11.7663 |          1250.76 | 270636 |     29 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mResult for PPO_MultiAgentBattlesnake-v1_2c7616ac:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-07-16_21-42-45\n",
      "  done: true\n",
      "  episode_len_mean: 95.51\n",
      "  episode_reward_max: 28.650000000000258\n",
      "  episode_reward_mean: 11.933500000000038\n",
      "  episode_reward_min: -0.7999999999999998\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 20550\n",
      "  experiment_id: 04b4be94a18f47938b20d396475d97f6\n",
      "  experiment_tag: '0'\n",
      "  hostname: ip-10-0-157-251.us-west-2.compute.internal\n",
      "  info:\n",
      "    grad_time_ms: 10601.319\n",
      "    learner:\n",
      "      policy_0:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.6823322772979736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007734912913292646\n",
      "        policy_loss: -0.016199372708797455\n",
      "        total_loss: 59.76768112182617\n",
      "        vf_explained_var: 0.910664975643158\n",
      "        vf_loss: 59.78126907348633\n",
      "      policy_1:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.46004927158355713\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008969059213995934\n",
      "        policy_loss: -0.011223167181015015\n",
      "        total_loss: 69.11310577392578\n",
      "        vf_explained_var: 0.9213516712188721\n",
      "        vf_loss: 69.12128448486328\n",
      "      policy_2:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.42189428210258484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006956554017961025\n",
      "        policy_loss: -0.01231391541659832\n",
      "        total_loss: 44.873470306396484\n",
      "        vf_explained_var: 0.9036532640457153\n",
      "        vf_loss: 44.88108825683594\n",
      "      policy_3:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.551855206489563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011785609647631645\n",
      "        policy_loss: -0.01436932384967804\n",
      "        total_loss: 62.84931182861328\n",
      "        vf_explained_var: 0.90382981300354\n",
      "        vf_loss: 62.85970687866211\n",
      "      policy_4:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        entropy: 0.3628143072128296\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005164024885743856\n",
      "        policy_loss: -0.014305027201771736\n",
      "        total_loss: 58.70368957519531\n",
      "        vf_explained_var: 0.9130786061286926\n",
      "        vf_loss: 58.71537780761719\n",
      "    load_time_ms: 668.4\n",
      "    num_steps_sampled: 279987\n",
      "    num_steps_trained: 277248\n",
      "    sample_time_ms: 29873.603\n",
      "    update_time_ms: 21.193\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 10.0.157.251\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.06730769230771\n",
      "    ram_util_percent: 49.357692307692304\n",
      "  pid: 114\n",
      "  policy_reward_max:\n",
      "    policy_0: 12.74999999999998\n",
      "    policy_1: 12.449999999999982\n",
      "    policy_2: 12.89999999999998\n",
      "    policy_3: 13.84999999999999\n",
      "    policy_4: 10.649999999999988\n",
      "  policy_reward_mean:\n",
      "    policy_0: 1.8284999999999971\n",
      "    policy_1: 2.5199999999999965\n",
      "    policy_2: 2.7899999999999956\n",
      "    policy_3: 2.134999999999996\n",
      "    policy_4: 2.6599999999999953\n",
      "  policy_reward_min:\n",
      "    policy_0: -1.95\n",
      "    policy_1: -1.9\n",
      "    policy_2: -1.95\n",
      "    policy_3: -1.95\n",
      "    policy_4: -1.95\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 1.746011103141961\n",
      "    mean_inference_ms: 6.33135699821748\n",
      "    mean_processing_ms: 1.295494852311484\n",
      "  sm__effective_map_size: 11\n",
      "  sm__episode_len_mean: 95.51\n",
      "  sm__episode_reward_max: 28.650000000000258\n",
      "  sm__episode_reward_mean: 11.933500000000038\n",
      "  sm__episode_reward_min: -0.7999999999999998\n",
      "  sm__policy_0_reward_max: 12.74999999999998\n",
      "  sm__policy_0_reward_mean: 1.8284999999999971\n",
      "  sm__policy_0_reward_min: -1.95\n",
      "  time_since_restore: 1291.9189867973328\n",
      "  time_this_iter_s: 41.16317796707153\n",
      "  time_total_s: 1291.9189867973328\n",
      "  timestamp: 1594935765\n",
      "  timesteps_since_restore: 279987\n",
      "  timesteps_this_iter: 9351\n",
      "  timesteps_total: 279987\n",
      "  training_iteration: 30\n",
      "  trial_id: 2c7616ac\n",
      "  \u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 7.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 4/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 RUNNING)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status   | loc              |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+----------+------------------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | RUNNING  | 10.0.157.251:114 |  11.9335 |          1291.92 | 279987 |     30 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+----------+------------------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m== Status ==\u001b[0m\n",
      "\u001b[34mMemory usage on this node: 7.3/15.2 GiB\u001b[0m\n",
      "\u001b[34mUsing FIFO scheduling algorithm.\u001b[0m\n",
      "\u001b[34mResources requested: 0/4 CPUs, 0/0 GPUs, 0.0/6.69 GiB heap, 0.0/2.29 GiB objects\u001b[0m\n",
      "\u001b[34mResult logdir: /opt/ml/output/intermediate/training\u001b[0m\n",
      "\u001b[34mNumber of trials: 1 (1 TERMINATED)\u001b[0m\n",
      "\u001b[34m+---------------------------------------+------------+-------+----------+------------------+--------+--------+\u001b[0m\n",
      "\u001b[34m| Trial name                            | status     | loc   |   reward |   total time (s) |     ts |   iter |\u001b[0m\n",
      "\u001b[34m|---------------------------------------+------------+-------+----------+------------------+--------+--------|\u001b[0m\n",
      "\u001b[34m| PPO_MultiAgentBattlesnake-v1_2c7616ac | TERMINATED |       |  11.9335 |          1291.92 | 279987 |     30 |\u001b[0m\n",
      "\u001b[34m+---------------------------------------+------------+-------+----------+------------------+--------+--------+\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mSaved model configuration.\u001b[0m\n",
      "\u001b[34mSaved the checkpoint file /opt/ml/output/intermediate/training/PPO_MultiAgentBattlesnake-v1_2c7616ac_0_2020-07-16_21-20-2919p4rm_5/checkpoint_30/checkpoint-30 as /opt/ml/model/checkpoint\u001b[0m\n",
      "\u001b[34mSaved the checkpoint file /opt/ml/output/intermediate/training/PPO_MultiAgentBattlesnake-v1_2c7616ac_0_2020-07-16_21-20-2919p4rm_5/checkpoint_30/checkpoint-30.tune_metadata as /opt/ml/model/checkpoint.tune_metadata\u001b[0m\n",
      "\u001b[34m2020-07-16 21:42:51,109#011INFO trainer.py:420 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\u001b[0m\n",
      "\u001b[34m2020-07-16 21:42:51,116#011INFO trainer.py:580 -- Current log_level is ERROR. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=7247)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=7247)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=7247)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=7247)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=7247)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=7247)#033[0m   obj = yaml.load(type_)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#033[2m#033[36m(pid=7247)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=7247)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=7247)#033[0m /usr/local/lib/python3.6/dist-packages/ray/rllib/utils/from_config.py:134: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\u001b[0m\n",
      "\u001b[34m#033[2m#033[36m(pid=7247)#033[0m   obj = yaml.load(type_)\u001b[0m\n",
      "\u001b[34m2020-07-16 21:43:20,985#011INFO trainable.py:178 -- _setup took 29.871 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\u001b[0m\n",
      "\u001b[34m2020-07-16 21:43:22,781#011WARNING trainable.py:210 -- Getting current IP.\u001b[0m\n",
      "\u001b[34m2020-07-16 21:43:22,782#011INFO trainable.py:416 -- Restored on 10.0.157.251 from checkpoint: /opt/ml/model/checkpoint\u001b[0m\n",
      "\u001b[34m2020-07-16 21:43:22,782#011INFO trainable.py:423 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': 279987, '_time_total': 1291.9189867973328, '_episodes_total': 20550}\u001b[0m\n",
      "\n",
      "2020-07-16 21:43:40 Uploading - Uploading generated training model\n",
      "2020-07-16 21:43:40 Completed - Training job completed\n",
      "\u001b[34mSaved TensorFlow serving model!\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-16 21:43:27,910 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 1473\n",
      "Billable seconds: 1473\n",
      "Training job: Battlesnake-job-rllib-2020-07-16-21-17-19-254\n",
      "CPU times: user 3.73 s, sys: 215 ms, total: 3.94 s\n",
      "Wall time: 26min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define and execute our training job\n",
    "# Adjust hyperparameters and train_instance_count accordingly\n",
    "\n",
    "metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)\n",
    "    \n",
    "estimator = RLEstimator(entry_point=\"train-mabs.py\",\n",
    "                        source_dir='rllib_src',\n",
    "                        dependencies=[\"rllib_common/sagemaker_rl\", \"battlesnake_gym/\"],\n",
    "                        image_name=image_name,\n",
    "                        role=role,\n",
    "                        train_instance_type=instance_type,\n",
    "                        train_instance_count=1,\n",
    "                        output_path=s3_output_path,\n",
    "                        base_job_name=job_name_prefix,\n",
    "                        metric_definitions=metric_definitions,\n",
    "                        hyperparameters={\n",
    "                            # See train-mabs.py to add additional hyperparameters\n",
    "                            # Also see ray_launcher.py for the rl.training.* hyperparameters\n",
    "                            #\n",
    "                            # number of training iterations\n",
    "                            \"num_iters\": 30,\n",
    "                            # number of snakes in the gym\n",
    "                            \"num_agents\": 5,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "estimator.fit()\n",
    "\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-216604823851/Battlesnake-job-rllib-2020-07-16-21-17-19-254/output/model.tar.gz'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where is the model stored in S3?\n",
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an endpoint to host the policy\n",
    "Firstly, we will delete the previous endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName='battlesnake-endpoint')\n",
    "sm_client.delete_endpoint_config(EndpointConfigName='battlesnake-endpoint')\n",
    "sm_client.delete_model(ModelName=\"battlesnake-rllib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy the endpoint to a central location\n",
    "model_data = \"s3://{}/battlesnake-aws/pretrainedmodels/model.tar.gz\".format(s3_bucket)\n",
    "!aws s3 cp {estimator.model_data} {model_data}\n",
    "\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "model = Model(model_data=model_data,\n",
    "              role=role,\n",
    "              entry_point=\"inference.py\",\n",
    "              source_dir='rllib_inference/src',\n",
    "              framework_version='2.1.0',\n",
    "              name=\"battlesnake-rllib\",\n",
    "             )\n",
    "\n",
    "if local_mode:\n",
    "    inf_instance_type = 'local'\n",
    "else:\n",
    "    inf_instance_type = \"ml.t2.medium\"\n",
    "\n",
    "# Deploy an inference endpoint\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=inf_instance_type,#instance_type=\"local\", #\n",
    "                         endpoint_name='battlesnake-endpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the endpoint\n",
    "\n",
    "This example is using single observation for a 5-agent environment \n",
    "The last axis is 12 because the current MultiAgentEnv is concatenating 2 frames\n",
    "5 agent maps + 1 food map = 6 maps total    6 maps * 2 frames = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "data1 = np.zeros(shape=(1, 21, 21, 6), dtype=np.float32).tolist()\n",
    "\n",
    "health_dict = {0: 50, 1: 50}\n",
    "json = {\"turn\": 4,\n",
    "        \"board\": {\n",
    "                \"height\": 15,\n",
    "                \"width\": 15,\n",
    "                \"food\": [],\n",
    "                \"snakes\": []\n",
    "                },\n",
    "            \"you\": {\n",
    "                \"id\": \"snake-id-string\",\n",
    "                \"name\": \"Sneky Snek\",\n",
    "                \"health\": 90,\n",
    "                \"body\": [{\"x\": 1, \"y\": 3}]\n",
    "                }\n",
    "            }\n",
    "\n",
    "before = time()\n",
    "action = predictor.predict({\"state\": data1, \"prev_action\": -1, \n",
    "                           \"prev_reward\": -1, \"seq_lens\": -1,  \n",
    "                           \"all_health\": health_dict, \"json\": json})\n",
    "elapsed = time() - before\n",
    "\n",
    "action_to_take = action[\"outputs\"][\"heuristisc_action\"]\n",
    "print(\"Action to take {}\".format(action_to_take))\n",
    "print(\"Inference took %.2f ms\" % (elapsed*1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
